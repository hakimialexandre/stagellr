{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison on different algorithms for electron/pion discrimination\n",
    "\n",
    "\n",
    "## 1. Introduction  \n",
    "\n",
    "The goal of this study is to compare the identification perfomance of different types of algorithms for discriminating electrons and pions in HGCAL. This analysis is made by training BDT's on the simulated DATA obtained through those algorithms.  \n",
    "Here are the algorithms that we will compare:  \n",
    "* ThresholdRef2dRef3dGenclusters (T23) is the old algorithm used by creating 2D clusters first then reconstructing 3D clusters\n",
    "* SupertriggercellDummyHistomaxClusters (SDH)\n",
    "* SupertriggercellDummyHistomaxvardrth0Clusters (SDH0)\n",
    "* SupertriggercellDummyHistomaxvardrth10Clusters (SDH10)\n",
    "* SupertriggercellDummyHistomaxvardrth20Clusters (SDH20)\n",
    "* ThresholdDummyHistomaxClusters (TDH)\n",
    "* ThresholdDummyHistomaxvardrth0 (TDH0)\n",
    "* ThresholdDummyHistomaxvardrth10 (TDH10)\n",
    "* ThresholdDummyHistomaxvardrth20 (TDH20)\n",
    "\n",
    "The data are found in root files at <https://cernbox.cern.ch/index.php/s/Mr9t9Fn4VyDfB4p>\n",
    "\n",
    "## 2. Preprocessing\n",
    "The files need to be preprocessed. The code to do so is found in this [notebook](preprocessing.ipynb). For each algorithm there is a file (actually multiple files that need to be fused) containing multiple entries. Each entry corresponds to a cluster reconstructed by the algorithms with several variables noted 'cl3d_\\*'. In the T23 file are also entries corresponding to the true generated DATA.\n",
    "\n",
    "The raw files contain an entry for every particle created during the simulation. The first step of preprocessing is then to cut every entry for which the particle was not one of the two mother particles or didn't reach the endcap.  \n",
    "Once this is done, we proceed to match the reconstructed clusters to a generated particle. To this end, we calculate the deltar for the between the generatedd particle and every candidate cluster, then select the highest pt candidate with deltar \\< threshold (0.05)\n",
    "\n",
    "## 3. Analysis and BDT\n",
    "The BDT analysis is realised in this [notebook](BDT.ipynb).\n",
    "\n",
    "Before training our BDTs, we cut our data under 10 pt and keep only 1.6 \\< eta\\< 2.9 to only use the clearest data, the reconstruction efficiency falling under those cuts.\n",
    "\n",
    "Then, we get a more in-depth look at the distribution of the variables of interest for us (those we will use as features for BDT):\n",
    "* eta\n",
    "* longitudinal variables: shower length, core shower length, first layer, maw layer, szztot\n",
    "* form factors: srrmean, srrtot, seetot, spptot  \n",
    "\n",
    "Here are the correlation matrices for signal (electron) and background (pions) for every algorithms:\n",
    "![T23](corr_matrix/correlation_matrix_T23.png)\n",
    "![SDH](corr_matrix/correlation_matrix_SDH.png)\n",
    "![SDH0](corr_matrix/correlation_matrix_SDH0.png)\n",
    "![SDH10](corr_matrix/correlation_matrix_SDH10.png)\n",
    "![SDH20](corr_matrix/correlation_matrix_SDH20.png)\n",
    "![TDH](corr_matrix/correlation_matrix_TDH.png)\n",
    "![TDH0](corr_matrix/correlation_matrix_TDH0.png)\n",
    "![TDH10](corr_matrix/correlation_matrix_TDH10.png)\n",
    "![TDH20](corr_matrix/correlation_matrix_TDH20.png)\n",
    "\n",
    "And violin plots representing the distribution of the variables across the differents algorithms for electrons and pions:  \n",
    "![violin](violinplotall.png)\n",
    "\n",
    "\n",
    "We can now train our BDT using XGBoost library and the following parameters:\n",
    "    \n",
    "    param['nthread']          = 30  # limit number of threads\n",
    "    param['eta']              = 0.1 # learning rate\n",
    "    param['max_depth']        = 10  # maximum depth of a tree\n",
    "    param['subsample']        = 0.8 # fraction of events to train tree on\n",
    "    param['colsample_bytree'] = 0.8 # fraction of features to train tree on\n",
    "    param['silent'] = True\n",
    "    param['objective']   = 'binary:logistic' # objective function\n",
    "    param['eval_metric'] = 'error'           # evaluation metric for cross validation\n",
    "    param = list(param.items()) + [('eval_metric', 'logloss')] + [('eval_metric', 'rmse')]\n",
    "    num_trees = 100  # number of trees to make\n",
    "\n",
    "We can now take a look at the predictions from the BDTs against true value;\n",
    "![prediction](predictions.png)\n",
    "And the imporantce of each variable (as mode: gain to show the discriminating power of the variable)\n",
    "![importance](importance.png)\n",
    "\n",
    "We can now plot the ROC curve for the algorithms:\n",
    "![roc](ROC.png)\n",
    "We can see that without Pileup, the efficiency is extremely high, we then pick a working point of 0.995 efficiency that is way higher than whate we expect to keep in heavy pileup situation.\n",
    "\n",
    "To compare the algorithms we decide to use the background efficiency as scoring: at our working point, it is the percentage of backgroung (here pions) that are incorrectly identified as signal, closest to 0 is better)\n",
    "![score](efficiency.png)\n",
    "\n",
    "As a bonus, the T23 files contained old data for BDT trained with other methods, here is a comparison between our results and those\n",
    "![old](old.png)\n",
    "\n",
    "\n",
    "We now want ot take in depth look at our BDT efficiency vs bins of pt and eta, with error bars corresponding to the clopper-pearson measur of uncertainties for a confidence level of 99% and a working point of 0.95 (more in agreement to what we expect to keep in presence of pilup). Theplots are rendered raw and with logarithmic scale to better see the close 0 variations:\n",
    "![efficiency](Algo_comparison.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

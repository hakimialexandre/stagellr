{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "import xgboost as xgb\n",
    "import torch\n",
    "import torch.utils.data as data_utils\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "workdir=os.getcwd()\n",
    "os.makedirs('fig', exist_ok=True)\n",
    "fig_dir=workdir+'/fig'\n",
    "algo_name=['T23', 'S', 'S0', 'S10', 'S20', 'T', 'T0', 'T10', 'T20']\n",
    "algo={}\n",
    "for i in range(len(algo_name)):\n",
    "    algo[i]=pd.read_csv(workdir+'/data/{}.csv'.format(algo_name[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#cut\n",
    "ptcut=10\n",
    "etamin=1.6\n",
    "etamax=2.9\n",
    "algo_cut={}\n",
    "for i in algo:\n",
    "    sel=algo[i]['genpart_pt']>ptcut\n",
    "    algo_cut[i]=algo[i][sel]\n",
    "    sel=np.abs(algo_cut[i]['genpart_exeta'])>etamin\n",
    "    algo_cut[i]=algo_cut[i][sel]\n",
    "    sel=np.abs(algo_cut[i]['genpart_exeta'])<etamax\n",
    "    algo_cut[i]=algo_cut[i][sel]\n",
    "    algo_cut[i].dropna(inplace=True)\n",
    "    algo_cut[i]['genpart_pid'].replace([-11,11],0, inplace=True)\n",
    "    algo_cut[i]['genpart_pid'].replace([-211,211],1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Keep only relevant features and split the data between train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "columns=['cl3d_eta','cl3d_showerlength',\n",
    "       'cl3d_coreshowerlength', 'cl3d_firstlayer', 'cl3d_maxlayer', 'cl3d_szz',\n",
    "       'cl3d_seetot', 'cl3d_spptot', 'cl3d_srrtot', 'cl3d_srrmean','cl3d_pt']\n",
    "\n",
    "\n",
    "X_train={}\n",
    "X_test={}\n",
    "y_train={}\n",
    "y_test={}\n",
    "\n",
    "for i in algo:\n",
    "    X_train[i], X_test[i], y_train[i], y_test[i] = train_test_split(algo_cut[i][columns], algo_cut[i]['genpart_pid'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep the pt information for later\n",
    "X_pt={}\n",
    "for i in algo:\n",
    "    X_pt[i]=X_test[i]['cl3d_pt']\n",
    "    X_test[i]=X_test[i].drop(columns='cl3d_pt')\n",
    "    X_train[i]=X_train[i].drop(columns='cl3d_pt')\n",
    "    \n",
    "columns.remove('cl3d_pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NN parameters\n",
    "device = torch.device(\"cpu\")\n",
    "#device = torch.device(\"cuda:0\") \n",
    "\n",
    "D_in =X_train[1].shape[0]  # input dimension = number of features\n",
    "H = 10 # Hidden layer dimension\n",
    "D_out = 2 # Output dimension= number of classes\n",
    "batch_size= 100\n",
    "n_epochs = 100\n",
    "learning_rate= 1e-6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "import torch.utils.data as data_utils\n",
    "tensor_train={}\n",
    "tensor_test={}\n",
    "loader_train={}\n",
    "loader_test={}\n",
    "\n",
    "for i in algo:\n",
    "    tensor_train[i] = data_utils.TensorDataset(torch.tensor(X_train[i].values, dtype=torch.float), torch.tensor(y_train[i].values, dtype=torch.long))\n",
    "    tensor_test[i] = data_utils.TensorDataset(torch.tensor(X_test[i].values, dtype=torch.float), torch.tensor(y_test[i].values, dtype=torch.long))\n",
    "    loader_train[i] = data_utils.DataLoader(tensor_train[i], batch_size=batch_size, shuffle=False)\n",
    "    loader_test[i] = data_utils.DataLoader(tensor_test[i], batch_size=batch_size, shuffle=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [100/15040], Loss: 1.0211\n",
      "Epoch [1/100], Step [200/15040], Loss: 0.9298\n",
      "Epoch [1/100], Step [300/15040], Loss: 0.9754\n",
      "Epoch [1/100], Step [400/15040], Loss: 0.9604\n",
      "Epoch [1/100], Step [500/15040], Loss: 0.9408\n",
      "Epoch [1/100], Step [600/15040], Loss: 1.0117\n",
      "Epoch [1/100], Step [700/15040], Loss: 0.9266\n",
      "Epoch [1/100], Step [800/15040], Loss: 0.8722\n",
      "Epoch [1/100], Step [900/15040], Loss: 0.8197\n",
      "Epoch [1/100], Step [1000/15040], Loss: 1.1364\n",
      "Epoch [1/100], Step [1100/15040], Loss: 0.9909\n",
      "Epoch [1/100], Step [1200/15040], Loss: 0.8628\n",
      "Epoch [1/100], Step [1300/15040], Loss: 0.9418\n",
      "Epoch [1/100], Step [1400/15040], Loss: 0.8676\n",
      "Epoch [1/100], Step [1500/15040], Loss: 0.8995\n",
      "Epoch [1/100], Step [1600/15040], Loss: 0.9398\n",
      "Epoch [1/100], Step [1700/15040], Loss: 0.9117\n",
      "Epoch [1/100], Step [1800/15040], Loss: 0.7950\n",
      "Epoch [1/100], Step [1900/15040], Loss: 0.8448\n",
      "Epoch [1/100], Step [2000/15040], Loss: 0.8536\n",
      "Epoch [1/100], Step [2100/15040], Loss: 0.9181\n",
      "Epoch [1/100], Step [2200/15040], Loss: 0.8581\n",
      "Epoch [1/100], Step [2300/15040], Loss: 0.8901\n",
      "Epoch [1/100], Step [2400/15040], Loss: 0.8135\n",
      "Epoch [1/100], Step [2500/15040], Loss: 0.7089\n",
      "Epoch [1/100], Step [2600/15040], Loss: 0.8272\n",
      "Epoch [1/100], Step [2700/15040], Loss: 0.7850\n",
      "Epoch [1/100], Step [2800/15040], Loss: 0.8355\n",
      "Epoch [1/100], Step [2900/15040], Loss: 0.8137\n",
      "Epoch [1/100], Step [3000/15040], Loss: 0.8012\n",
      "Epoch [1/100], Step [3100/15040], Loss: 0.7685\n",
      "Epoch [1/100], Step [3200/15040], Loss: 0.8236\n",
      "Epoch [1/100], Step [3300/15040], Loss: 0.7226\n",
      "Epoch [1/100], Step [3400/15040], Loss: 0.8310\n",
      "Epoch [1/100], Step [3500/15040], Loss: 0.7630\n",
      "Epoch [1/100], Step [3600/15040], Loss: 0.7245\n",
      "Epoch [1/100], Step [3700/15040], Loss: 0.8267\n",
      "Epoch [1/100], Step [3800/15040], Loss: 0.5876\n",
      "Epoch [1/100], Step [3900/15040], Loss: 0.6701\n",
      "Epoch [1/100], Step [4000/15040], Loss: 0.7635\n",
      "Epoch [1/100], Step [4100/15040], Loss: 0.6776\n",
      "Epoch [1/100], Step [4200/15040], Loss: 0.8476\n",
      "Epoch [1/100], Step [4300/15040], Loss: 0.7453\n",
      "Epoch [1/100], Step [4400/15040], Loss: 0.6394\n",
      "Epoch [1/100], Step [4500/15040], Loss: 0.6958\n",
      "Epoch [1/100], Step [4600/15040], Loss: 0.6913\n",
      "Epoch [1/100], Step [4700/15040], Loss: 0.6583\n",
      "Epoch [1/100], Step [4800/15040], Loss: 0.6110\n",
      "Epoch [1/100], Step [4900/15040], Loss: 0.6079\n",
      "Epoch [1/100], Step [5000/15040], Loss: 0.7006\n",
      "Epoch [1/100], Step [5100/15040], Loss: 0.5880\n",
      "Epoch [1/100], Step [5200/15040], Loss: 0.6308\n",
      "Epoch [1/100], Step [5300/15040], Loss: 0.6365\n",
      "Epoch [1/100], Step [5400/15040], Loss: 0.7570\n",
      "Epoch [1/100], Step [5500/15040], Loss: 0.6747\n",
      "Epoch [1/100], Step [5600/15040], Loss: 0.5821\n",
      "Epoch [1/100], Step [5700/15040], Loss: 0.6624\n",
      "Epoch [1/100], Step [5800/15040], Loss: 0.6911\n",
      "Epoch [1/100], Step [5900/15040], Loss: 0.5570\n",
      "Epoch [1/100], Step [6000/15040], Loss: 0.7494\n",
      "Epoch [1/100], Step [6100/15040], Loss: 0.4800\n",
      "Epoch [1/100], Step [6200/15040], Loss: 0.7889\n",
      "Epoch [1/100], Step [6300/15040], Loss: 0.5983\n",
      "Epoch [1/100], Step [6400/15040], Loss: 0.5228\n",
      "Epoch [1/100], Step [6500/15040], Loss: 0.5033\n",
      "Epoch [1/100], Step [6600/15040], Loss: 0.6434\n",
      "Epoch [1/100], Step [6700/15040], Loss: 0.6183\n",
      "Epoch [1/100], Step [6800/15040], Loss: 0.7180\n",
      "Epoch [1/100], Step [6900/15040], Loss: 0.5702\n",
      "Epoch [1/100], Step [7000/15040], Loss: 0.5856\n",
      "Epoch [1/100], Step [7100/15040], Loss: 0.5451\n",
      "Epoch [1/100], Step [7200/15040], Loss: 0.6396\n",
      "Epoch [1/100], Step [7300/15040], Loss: 0.5747\n",
      "Epoch [1/100], Step [7400/15040], Loss: 0.4985\n",
      "Epoch [1/100], Step [7500/15040], Loss: 0.7421\n",
      "Epoch [1/100], Step [7600/15040], Loss: 0.5859\n",
      "Epoch [1/100], Step [7700/15040], Loss: 0.7472\n",
      "Epoch [1/100], Step [7800/15040], Loss: 0.5413\n",
      "Epoch [1/100], Step [7900/15040], Loss: 0.4885\n",
      "Epoch [1/100], Step [8000/15040], Loss: 0.5146\n",
      "Epoch [1/100], Step [8100/15040], Loss: 0.6867\n",
      "Epoch [1/100], Step [8200/15040], Loss: 0.5477\n",
      "Epoch [1/100], Step [8300/15040], Loss: 0.7511\n",
      "Epoch [1/100], Step [8400/15040], Loss: 0.5718\n",
      "Epoch [1/100], Step [8500/15040], Loss: 0.5429\n",
      "Epoch [1/100], Step [8600/15040], Loss: 0.5799\n",
      "Epoch [1/100], Step [8700/15040], Loss: 0.5688\n",
      "Epoch [1/100], Step [8800/15040], Loss: 0.6229\n",
      "Epoch [1/100], Step [8900/15040], Loss: 0.7146\n",
      "Epoch [1/100], Step [9000/15040], Loss: 0.6034\n",
      "Epoch [1/100], Step [9100/15040], Loss: 0.5347\n",
      "Epoch [1/100], Step [9200/15040], Loss: 0.4356\n",
      "Epoch [1/100], Step [9300/15040], Loss: 0.5843\n",
      "Epoch [1/100], Step [9400/15040], Loss: 0.5211\n",
      "Epoch [1/100], Step [9500/15040], Loss: 0.7348\n",
      "Epoch [1/100], Step [9600/15040], Loss: 0.4453\n",
      "Epoch [1/100], Step [9700/15040], Loss: 0.3837\n",
      "Epoch [1/100], Step [9800/15040], Loss: 0.6270\n",
      "Epoch [1/100], Step [9900/15040], Loss: 0.4850\n",
      "Epoch [1/100], Step [10000/15040], Loss: 0.6621\n",
      "Epoch [1/100], Step [10100/15040], Loss: 0.7189\n",
      "Epoch [1/100], Step [10200/15040], Loss: 0.4433\n",
      "Epoch [1/100], Step [10300/15040], Loss: 0.5929\n",
      "Epoch [1/100], Step [10400/15040], Loss: 0.5336\n",
      "Epoch [1/100], Step [10500/15040], Loss: 0.4488\n",
      "Epoch [1/100], Step [10600/15040], Loss: 0.4697\n",
      "Epoch [1/100], Step [10700/15040], Loss: 0.6374\n",
      "Epoch [1/100], Step [10800/15040], Loss: 0.5387\n",
      "Epoch [1/100], Step [10900/15040], Loss: 0.6998\n",
      "Epoch [1/100], Step [11000/15040], Loss: 0.6148\n",
      "Epoch [1/100], Step [11100/15040], Loss: 0.6859\n",
      "Epoch [1/100], Step [11200/15040], Loss: 0.6827\n",
      "Epoch [1/100], Step [11300/15040], Loss: 0.6929\n",
      "Epoch [1/100], Step [11400/15040], Loss: 0.3529\n",
      "Epoch [1/100], Step [11500/15040], Loss: 0.5681\n",
      "Epoch [1/100], Step [11600/15040], Loss: 0.4852\n",
      "Epoch [1/100], Step [11700/15040], Loss: 0.4512\n",
      "Epoch [1/100], Step [11800/15040], Loss: 0.5490\n",
      "Epoch [1/100], Step [11900/15040], Loss: 0.5553\n",
      "Epoch [1/100], Step [12000/15040], Loss: 0.4286\n",
      "Epoch [1/100], Step [12100/15040], Loss: 0.6328\n",
      "Epoch [1/100], Step [12200/15040], Loss: 0.6004\n",
      "Epoch [1/100], Step [12300/15040], Loss: 0.4185\n",
      "Epoch [1/100], Step [12400/15040], Loss: 0.5644\n",
      "Epoch [1/100], Step [12500/15040], Loss: 0.4386\n",
      "Epoch [1/100], Step [12600/15040], Loss: 0.4588\n",
      "Epoch [1/100], Step [12700/15040], Loss: 0.7662\n",
      "Epoch [1/100], Step [12800/15040], Loss: 0.5439\n",
      "Epoch [1/100], Step [12900/15040], Loss: 0.4018\n",
      "Epoch [1/100], Step [13000/15040], Loss: 0.5873\n",
      "Epoch [1/100], Step [13100/15040], Loss: 0.7037\n",
      "Epoch [1/100], Step [13200/15040], Loss: 0.6512\n",
      "Epoch [1/100], Step [13300/15040], Loss: 0.5077\n",
      "Epoch [1/100], Step [13400/15040], Loss: 0.4007\n",
      "Epoch [1/100], Step [13500/15040], Loss: 0.4702\n",
      "Epoch [1/100], Step [13600/15040], Loss: 0.5818\n",
      "Epoch [1/100], Step [13700/15040], Loss: 0.6416\n",
      "Epoch [1/100], Step [13800/15040], Loss: 0.5609\n",
      "Epoch [1/100], Step [13900/15040], Loss: 0.4224\n",
      "Epoch [1/100], Step [14000/15040], Loss: 0.3212\n",
      "Epoch [1/100], Step [14100/15040], Loss: 0.6005\n",
      "Epoch [1/100], Step [14200/15040], Loss: 0.5325\n",
      "Epoch [1/100], Step [14300/15040], Loss: 0.4572\n",
      "Epoch [1/100], Step [14400/15040], Loss: 0.4761\n",
      "Epoch [1/100], Step [14500/15040], Loss: 0.7773\n",
      "Epoch [1/100], Step [14600/15040], Loss: 0.4320\n",
      "Epoch [1/100], Step [14700/15040], Loss: 0.6614\n",
      "Epoch [1/100], Step [14800/15040], Loss: 0.6946\n",
      "Epoch [1/100], Step [14900/15040], Loss: 0.6180\n",
      "Epoch [1/100], Step [15000/15040], Loss: 0.5657\n",
      "Epoch [2/100], Step [100/15040], Loss: 0.4956\n",
      "Epoch [2/100], Step [200/15040], Loss: 0.4166\n",
      "Epoch [2/100], Step [300/15040], Loss: 0.6497\n",
      "Epoch [2/100], Step [400/15040], Loss: 0.5352\n",
      "Epoch [2/100], Step [500/15040], Loss: 0.4388\n",
      "Epoch [2/100], Step [600/15040], Loss: 0.5657\n",
      "Epoch [2/100], Step [700/15040], Loss: 0.4630\n",
      "Epoch [2/100], Step [800/15040], Loss: 0.4276\n",
      "Epoch [2/100], Step [900/15040], Loss: 0.3186\n",
      "Epoch [2/100], Step [1000/15040], Loss: 0.6356\n",
      "Epoch [2/100], Step [1100/15040], Loss: 0.7653\n",
      "Epoch [2/100], Step [1200/15040], Loss: 0.4911\n",
      "Epoch [2/100], Step [1300/15040], Loss: 0.7677\n",
      "Epoch [2/100], Step [1400/15040], Loss: 0.5649\n",
      "Epoch [2/100], Step [1500/15040], Loss: 0.6402\n",
      "Epoch [2/100], Step [1600/15040], Loss: 0.6681\n",
      "Epoch [2/100], Step [1700/15040], Loss: 0.6964\n",
      "Epoch [2/100], Step [1800/15040], Loss: 0.4028\n",
      "Epoch [2/100], Step [1900/15040], Loss: 0.4392\n",
      "Epoch [2/100], Step [2000/15040], Loss: 0.6166\n",
      "Epoch [2/100], Step [2100/15040], Loss: 0.6580\n",
      "Epoch [2/100], Step [2200/15040], Loss: 0.7025\n",
      "Epoch [2/100], Step [2300/15040], Loss: 0.6456\n",
      "Epoch [2/100], Step [2400/15040], Loss: 0.4463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100], Step [2500/15040], Loss: 0.3730\n",
      "Epoch [2/100], Step [2600/15040], Loss: 0.6171\n",
      "Epoch [2/100], Step [2700/15040], Loss: 0.4514\n",
      "Epoch [2/100], Step [2800/15040], Loss: 0.5836\n",
      "Epoch [2/100], Step [2900/15040], Loss: 0.5572\n",
      "Epoch [2/100], Step [3000/15040], Loss: 0.5356\n",
      "Epoch [2/100], Step [3100/15040], Loss: 0.3748\n",
      "Epoch [2/100], Step [3200/15040], Loss: 0.6272\n",
      "Epoch [2/100], Step [3300/15040], Loss: 0.4037\n",
      "Epoch [2/100], Step [3400/15040], Loss: 0.5587\n",
      "Epoch [2/100], Step [3500/15040], Loss: 0.4406\n",
      "Epoch [2/100], Step [3600/15040], Loss: 0.4930\n",
      "Epoch [2/100], Step [3700/15040], Loss: 0.6704\n",
      "Epoch [2/100], Step [3800/15040], Loss: 0.2722\n",
      "Epoch [2/100], Step [3900/15040], Loss: 0.3733\n",
      "Epoch [2/100], Step [4000/15040], Loss: 0.5018\n",
      "Epoch [2/100], Step [4100/15040], Loss: 0.3912\n",
      "Epoch [2/100], Step [4200/15040], Loss: 0.6326\n",
      "Epoch [2/100], Step [4300/15040], Loss: 0.4507\n",
      "Epoch [2/100], Step [4400/15040], Loss: 0.4358\n",
      "Epoch [2/100], Step [4500/15040], Loss: 0.5103\n",
      "Epoch [2/100], Step [4600/15040], Loss: 0.4449\n",
      "Epoch [2/100], Step [4700/15040], Loss: 0.3696\n",
      "Epoch [2/100], Step [4800/15040], Loss: 0.3899\n",
      "Epoch [2/100], Step [4900/15040], Loss: 0.3368\n",
      "Epoch [2/100], Step [5000/15040], Loss: 0.4889\n",
      "Epoch [2/100], Step [5100/15040], Loss: 0.3367\n",
      "Epoch [2/100], Step [5200/15040], Loss: 0.4332\n",
      "Epoch [2/100], Step [5300/15040], Loss: 0.3668\n",
      "Epoch [2/100], Step [5400/15040], Loss: 0.6097\n",
      "Epoch [2/100], Step [5500/15040], Loss: 0.5270\n",
      "Epoch [2/100], Step [5600/15040], Loss: 0.3826\n",
      "Epoch [2/100], Step [5700/15040], Loss: 0.4992\n",
      "Epoch [2/100], Step [5800/15040], Loss: 0.5304\n",
      "Epoch [2/100], Step [5900/15040], Loss: 0.3554\n",
      "Epoch [2/100], Step [6000/15040], Loss: 0.5324\n",
      "Epoch [2/100], Step [6100/15040], Loss: 0.2723\n",
      "Epoch [2/100], Step [6200/15040], Loss: 0.5898\n",
      "Epoch [2/100], Step [6300/15040], Loss: 0.4036\n",
      "Epoch [2/100], Step [6400/15040], Loss: 0.3350\n",
      "Epoch [2/100], Step [6500/15040], Loss: 0.3376\n",
      "Epoch [2/100], Step [6600/15040], Loss: 0.4929\n",
      "Epoch [2/100], Step [6700/15040], Loss: 0.4666\n",
      "Epoch [2/100], Step [6800/15040], Loss: 0.5366\n",
      "Epoch [2/100], Step [6900/15040], Loss: 0.4261\n",
      "Epoch [2/100], Step [7000/15040], Loss: 0.4215\n",
      "Epoch [2/100], Step [7100/15040], Loss: 0.3584\n",
      "Epoch [2/100], Step [7200/15040], Loss: 0.4692\n",
      "Epoch [2/100], Step [7300/15040], Loss: 0.4097\n",
      "Epoch [2/100], Step [7400/15040], Loss: 0.3134\n",
      "Epoch [2/100], Step [7500/15040], Loss: 0.6078\n",
      "Epoch [2/100], Step [7600/15040], Loss: 0.4378\n",
      "Epoch [2/100], Step [7700/15040], Loss: 0.6435\n",
      "Epoch [2/100], Step [7800/15040], Loss: 0.4337\n",
      "Epoch [2/100], Step [7900/15040], Loss: 0.3309\n",
      "Epoch [2/100], Step [8000/15040], Loss: 0.3706\n",
      "Epoch [2/100], Step [8100/15040], Loss: 0.5159\n",
      "Epoch [2/100], Step [8200/15040], Loss: 0.3785\n",
      "Epoch [2/100], Step [8300/15040], Loss: 0.6207\n",
      "Epoch [2/100], Step [8400/15040], Loss: 0.4115\n",
      "Epoch [2/100], Step [8500/15040], Loss: 0.4153\n",
      "Epoch [2/100], Step [8600/15040], Loss: 0.4441\n",
      "Epoch [2/100], Step [8700/15040], Loss: 0.4485\n",
      "Epoch [2/100], Step [8800/15040], Loss: 0.5004\n",
      "Epoch [2/100], Step [8900/15040], Loss: 0.5970\n",
      "Epoch [2/100], Step [9000/15040], Loss: 0.5055\n",
      "Epoch [2/100], Step [9100/15040], Loss: 0.3946\n",
      "Epoch [2/100], Step [9200/15040], Loss: 0.3206\n",
      "Epoch [2/100], Step [9300/15040], Loss: 0.4485\n",
      "Epoch [2/100], Step [9400/15040], Loss: 0.3712\n",
      "Epoch [2/100], Step [9500/15040], Loss: 0.6244\n",
      "Epoch [2/100], Step [9600/15040], Loss: 0.3244\n",
      "Epoch [2/100], Step [9700/15040], Loss: 0.2670\n",
      "Epoch [2/100], Step [9800/15040], Loss: 0.5356\n",
      "Epoch [2/100], Step [9900/15040], Loss: 0.3765\n",
      "Epoch [2/100], Step [10000/15040], Loss: 0.5453\n",
      "Epoch [2/100], Step [10100/15040], Loss: 0.6014\n",
      "Epoch [2/100], Step [10200/15040], Loss: 0.3383\n",
      "Epoch [2/100], Step [10300/15040], Loss: 0.4543\n",
      "Epoch [2/100], Step [10400/15040], Loss: 0.4039\n",
      "Epoch [2/100], Step [10500/15040], Loss: 0.3672\n",
      "Epoch [2/100], Step [10600/15040], Loss: 0.3691\n",
      "Epoch [2/100], Step [10700/15040], Loss: 0.5309\n",
      "Epoch [2/100], Step [10800/15040], Loss: 0.4399\n",
      "Epoch [2/100], Step [10900/15040], Loss: 0.5772\n",
      "Epoch [2/100], Step [11000/15040], Loss: 0.4762\n",
      "Epoch [2/100], Step [11100/15040], Loss: 0.5556\n",
      "Epoch [2/100], Step [11200/15040], Loss: 0.5420\n",
      "Epoch [2/100], Step [11300/15040], Loss: 0.5576\n",
      "Epoch [2/100], Step [11400/15040], Loss: 0.2547\n",
      "Epoch [2/100], Step [11500/15040], Loss: 0.4770\n",
      "Epoch [2/100], Step [11600/15040], Loss: 0.3835\n",
      "Epoch [2/100], Step [11700/15040], Loss: 0.3519\n",
      "Epoch [2/100], Step [11800/15040], Loss: 0.4338\n",
      "Epoch [2/100], Step [11900/15040], Loss: 0.4687\n",
      "Epoch [2/100], Step [12000/15040], Loss: 0.3407\n",
      "Epoch [2/100], Step [12100/15040], Loss: 0.5330\n",
      "Epoch [2/100], Step [12200/15040], Loss: 0.4907\n",
      "Epoch [2/100], Step [12300/15040], Loss: 0.3252\n",
      "Epoch [2/100], Step [12400/15040], Loss: 0.4551\n",
      "Epoch [2/100], Step [12500/15040], Loss: 0.3563\n",
      "Epoch [2/100], Step [12600/15040], Loss: 0.3688\n",
      "Epoch [2/100], Step [12700/15040], Loss: 0.6488\n",
      "Epoch [2/100], Step [12800/15040], Loss: 0.4235\n",
      "Epoch [2/100], Step [12900/15040], Loss: 0.3083\n",
      "Epoch [2/100], Step [13000/15040], Loss: 0.4601\n",
      "Epoch [2/100], Step [13100/15040], Loss: 0.5468\n",
      "Epoch [2/100], Step [13200/15040], Loss: 0.5263\n",
      "Epoch [2/100], Step [13300/15040], Loss: 0.3909\n",
      "Epoch [2/100], Step [13400/15040], Loss: 0.2972\n",
      "Epoch [2/100], Step [13500/15040], Loss: 0.3760\n",
      "Epoch [2/100], Step [13600/15040], Loss: 0.4761\n",
      "Epoch [2/100], Step [13700/15040], Loss: 0.5079\n",
      "Epoch [2/100], Step [13800/15040], Loss: 0.4597\n",
      "Epoch [2/100], Step [13900/15040], Loss: 0.3197\n",
      "Epoch [2/100], Step [14000/15040], Loss: 0.2545\n",
      "Epoch [2/100], Step [14100/15040], Loss: 0.4796\n",
      "Epoch [2/100], Step [14200/15040], Loss: 0.4371\n",
      "Epoch [2/100], Step [14300/15040], Loss: 0.3555\n",
      "Epoch [2/100], Step [14400/15040], Loss: 0.3653\n",
      "Epoch [2/100], Step [14500/15040], Loss: 0.6506\n",
      "Epoch [2/100], Step [14600/15040], Loss: 0.3591\n",
      "Epoch [2/100], Step [14700/15040], Loss: 0.5383\n",
      "Epoch [2/100], Step [14800/15040], Loss: 0.5439\n",
      "Epoch [2/100], Step [14900/15040], Loss: 0.5029\n",
      "Epoch [2/100], Step [15000/15040], Loss: 0.4790\n",
      "Epoch [3/100], Step [100/15040], Loss: 0.3915\n",
      "Epoch [3/100], Step [200/15040], Loss: 0.3395\n",
      "Epoch [3/100], Step [300/15040], Loss: 0.5181\n",
      "Epoch [3/100], Step [400/15040], Loss: 0.4464\n",
      "Epoch [3/100], Step [500/15040], Loss: 0.3673\n",
      "Epoch [3/100], Step [600/15040], Loss: 0.4510\n",
      "Epoch [3/100], Step [700/15040], Loss: 0.3746\n",
      "Epoch [3/100], Step [800/15040], Loss: 0.3554\n",
      "Epoch [3/100], Step [900/15040], Loss: 0.2616\n",
      "Epoch [3/100], Step [1000/15040], Loss: 0.5004\n",
      "Epoch [3/100], Step [1100/15040], Loss: 0.6032\n",
      "Epoch [3/100], Step [1200/15040], Loss: 0.3909\n",
      "Epoch [3/100], Step [1300/15040], Loss: 0.6037\n",
      "Epoch [3/100], Step [1400/15040], Loss: 0.4348\n",
      "Epoch [3/100], Step [1500/15040], Loss: 0.5350\n",
      "Epoch [3/100], Step [1600/15040], Loss: 0.5058\n",
      "Epoch [3/100], Step [1700/15040], Loss: 0.5604\n",
      "Epoch [3/100], Step [1800/15040], Loss: 0.3223\n",
      "Epoch [3/100], Step [1900/15040], Loss: 0.3439\n",
      "Epoch [3/100], Step [2000/15040], Loss: 0.4904\n",
      "Epoch [3/100], Step [2100/15040], Loss: 0.5253\n",
      "Epoch [3/100], Step [2200/15040], Loss: 0.5537\n",
      "Epoch [3/100], Step [2300/15040], Loss: 0.5187\n",
      "Epoch [3/100], Step [2400/15040], Loss: 0.3524\n",
      "Epoch [3/100], Step [2500/15040], Loss: 0.3048\n",
      "Epoch [3/100], Step [2600/15040], Loss: 0.4857\n",
      "Epoch [3/100], Step [2700/15040], Loss: 0.3636\n",
      "Epoch [3/100], Step [2800/15040], Loss: 0.4802\n",
      "Epoch [3/100], Step [2900/15040], Loss: 0.4561\n",
      "Epoch [3/100], Step [3000/15040], Loss: 0.4568\n",
      "Epoch [3/100], Step [3100/15040], Loss: 0.3159\n",
      "Epoch [3/100], Step [3200/15040], Loss: 0.4771\n",
      "Epoch [3/100], Step [3300/15040], Loss: 0.3469\n",
      "Epoch [3/100], Step [3400/15040], Loss: 0.4280\n",
      "Epoch [3/100], Step [3500/15040], Loss: 0.3464\n",
      "Epoch [3/100], Step [3600/15040], Loss: 0.3975\n",
      "Epoch [3/100], Step [3700/15040], Loss: 0.5068\n",
      "Epoch [3/100], Step [3800/15040], Loss: 0.2244\n",
      "Epoch [3/100], Step [3900/15040], Loss: 0.3045\n",
      "Epoch [3/100], Step [4000/15040], Loss: 0.3686\n",
      "Epoch [3/100], Step [4100/15040], Loss: 0.2973\n",
      "Epoch [3/100], Step [4200/15040], Loss: 0.4680\n",
      "Epoch [3/100], Step [4300/15040], Loss: 0.3426\n",
      "Epoch [3/100], Step [4400/15040], Loss: 0.3622\n",
      "Epoch [3/100], Step [4500/15040], Loss: 0.4043\n",
      "Epoch [3/100], Step [4600/15040], Loss: 0.3396\n",
      "Epoch [3/100], Step [4700/15040], Loss: 0.3035\n",
      "Epoch [3/100], Step [4800/15040], Loss: 0.2980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/100], Step [4900/15040], Loss: 0.2736\n",
      "Epoch [3/100], Step [5000/15040], Loss: 0.3888\n",
      "Epoch [3/100], Step [5100/15040], Loss: 0.2690\n",
      "Epoch [3/100], Step [5200/15040], Loss: 0.3290\n",
      "Epoch [3/100], Step [5300/15040], Loss: 0.2923\n",
      "Epoch [3/100], Step [5400/15040], Loss: 0.4663\n",
      "Epoch [3/100], Step [5500/15040], Loss: 0.3958\n",
      "Epoch [3/100], Step [5600/15040], Loss: 0.2958\n",
      "Epoch [3/100], Step [5700/15040], Loss: 0.3808\n",
      "Epoch [3/100], Step [5800/15040], Loss: 0.4057\n",
      "Epoch [3/100], Step [5900/15040], Loss: 0.2793\n",
      "Epoch [3/100], Step [6000/15040], Loss: 0.4099\n",
      "Epoch [3/100], Step [6100/15040], Loss: 0.2266\n",
      "Epoch [3/100], Step [6200/15040], Loss: 0.4585\n",
      "Epoch [3/100], Step [6300/15040], Loss: 0.3161\n",
      "Epoch [3/100], Step [6400/15040], Loss: 0.2829\n",
      "Epoch [3/100], Step [6500/15040], Loss: 0.2920\n",
      "Epoch [3/100], Step [6600/15040], Loss: 0.3815\n",
      "Epoch [3/100], Step [6700/15040], Loss: 0.3896\n",
      "Epoch [3/100], Step [6800/15040], Loss: 0.3938\n",
      "Epoch [3/100], Step [6900/15040], Loss: 0.3317\n",
      "Epoch [3/100], Step [7000/15040], Loss: 0.3353\n",
      "Epoch [3/100], Step [7100/15040], Loss: 0.2987\n",
      "Epoch [3/100], Step [7200/15040], Loss: 0.3440\n",
      "Epoch [3/100], Step [7300/15040], Loss: 0.3409\n",
      "Epoch [3/100], Step [7400/15040], Loss: 0.2419\n",
      "Epoch [3/100], Step [7500/15040], Loss: 0.4469\n",
      "Epoch [3/100], Step [7600/15040], Loss: 0.3472\n",
      "Epoch [3/100], Step [7700/15040], Loss: 0.5040\n",
      "Epoch [3/100], Step [7800/15040], Loss: 0.3368\n",
      "Epoch [3/100], Step [7900/15040], Loss: 0.2624\n",
      "Epoch [3/100], Step [8000/15040], Loss: 0.3200\n",
      "Epoch [3/100], Step [8100/15040], Loss: 0.3893\n",
      "Epoch [3/100], Step [8200/15040], Loss: 0.2901\n",
      "Epoch [3/100], Step [8300/15040], Loss: 0.4587\n",
      "Epoch [3/100], Step [8400/15040], Loss: 0.3341\n",
      "Epoch [3/100], Step [8500/15040], Loss: 0.3398\n",
      "Epoch [3/100], Step [8600/15040], Loss: 0.3592\n",
      "Epoch [3/100], Step [8700/15040], Loss: 0.3505\n",
      "Epoch [3/100], Step [8800/15040], Loss: 0.3932\n",
      "Epoch [3/100], Step [8900/15040], Loss: 0.4799\n",
      "Epoch [3/100], Step [9000/15040], Loss: 0.3802\n",
      "Epoch [3/100], Step [9100/15040], Loss: 0.3074\n",
      "Epoch [3/100], Step [9200/15040], Loss: 0.2712\n",
      "Epoch [3/100], Step [9300/15040], Loss: 0.3421\n",
      "Epoch [3/100], Step [9400/15040], Loss: 0.2853\n",
      "Epoch [3/100], Step [9500/15040], Loss: 0.4796\n",
      "Epoch [3/100], Step [9600/15040], Loss: 0.2502\n",
      "Epoch [3/100], Step [9700/15040], Loss: 0.2130\n",
      "Epoch [3/100], Step [9800/15040], Loss: 0.4030\n",
      "Epoch [3/100], Step [9900/15040], Loss: 0.3000\n",
      "Epoch [3/100], Step [10000/15040], Loss: 0.4159\n",
      "Epoch [3/100], Step [10100/15040], Loss: 0.4706\n",
      "Epoch [3/100], Step [10200/15040], Loss: 0.2947\n",
      "Epoch [3/100], Step [10300/15040], Loss: 0.3418\n",
      "Epoch [3/100], Step [10400/15040], Loss: 0.3021\n",
      "Epoch [3/100], Step [10500/15040], Loss: 0.2870\n",
      "Epoch [3/100], Step [10600/15040], Loss: 0.2971\n",
      "Epoch [3/100], Step [10700/15040], Loss: 0.4014\n",
      "Epoch [3/100], Step [10800/15040], Loss: 0.3434\n",
      "Epoch [3/100], Step [10900/15040], Loss: 0.4328\n",
      "Epoch [3/100], Step [11000/15040], Loss: 0.3613\n",
      "Epoch [3/100], Step [11100/15040], Loss: 0.4171\n",
      "Epoch [3/100], Step [11200/15040], Loss: 0.3981\n",
      "Epoch [3/100], Step [11300/15040], Loss: 0.4158\n",
      "Epoch [3/100], Step [11400/15040], Loss: 0.1953\n",
      "Epoch [3/100], Step [11500/15040], Loss: 0.3878\n",
      "Epoch [3/100], Step [11600/15040], Loss: 0.2862\n",
      "Epoch [3/100], Step [11700/15040], Loss: 0.2720\n",
      "Epoch [3/100], Step [11800/15040], Loss: 0.3363\n",
      "Epoch [3/100], Step [11900/15040], Loss: 0.3752\n",
      "Epoch [3/100], Step [12000/15040], Loss: 0.2704\n",
      "Epoch [3/100], Step [12100/15040], Loss: 0.4244\n",
      "Epoch [3/100], Step [12200/15040], Loss: 0.3918\n",
      "Epoch [3/100], Step [12300/15040], Loss: 0.2629\n",
      "Epoch [3/100], Step [12400/15040], Loss: 0.3545\n",
      "Epoch [3/100], Step [12500/15040], Loss: 0.2965\n",
      "Epoch [3/100], Step [12600/15040], Loss: 0.2935\n",
      "Epoch [3/100], Step [12700/15040], Loss: 0.5136\n",
      "Epoch [3/100], Step [12800/15040], Loss: 0.3124\n",
      "Epoch [3/100], Step [12900/15040], Loss: 0.2266\n",
      "Epoch [3/100], Step [13000/15040], Loss: 0.3218\n",
      "Epoch [3/100], Step [13100/15040], Loss: 0.4031\n",
      "Epoch [3/100], Step [13200/15040], Loss: 0.3969\n",
      "Epoch [3/100], Step [13300/15040], Loss: 0.2910\n",
      "Epoch [3/100], Step [13400/15040], Loss: 0.2237\n",
      "Epoch [3/100], Step [13500/15040], Loss: 0.2904\n",
      "Epoch [3/100], Step [13600/15040], Loss: 0.3715\n",
      "Epoch [3/100], Step [13700/15040], Loss: 0.3738\n",
      "Epoch [3/100], Step [13800/15040], Loss: 0.3534\n",
      "Epoch [3/100], Step [13900/15040], Loss: 0.2366\n",
      "Epoch [3/100], Step [14000/15040], Loss: 0.2087\n",
      "Epoch [3/100], Step [14100/15040], Loss: 0.3567\n",
      "Epoch [3/100], Step [14200/15040], Loss: 0.3534\n",
      "Epoch [3/100], Step [14300/15040], Loss: 0.2675\n",
      "Epoch [3/100], Step [14400/15040], Loss: 0.2759\n",
      "Epoch [3/100], Step [14500/15040], Loss: 0.5135\n",
      "Epoch [3/100], Step [14600/15040], Loss: 0.2967\n",
      "Epoch [3/100], Step [14700/15040], Loss: 0.4155\n",
      "Epoch [3/100], Step [14800/15040], Loss: 0.4019\n",
      "Epoch [3/100], Step [14900/15040], Loss: 0.3837\n",
      "Epoch [3/100], Step [15000/15040], Loss: 0.3863\n",
      "Epoch [4/100], Step [100/15040], Loss: 0.3021\n",
      "Epoch [4/100], Step [200/15040], Loss: 0.2736\n",
      "Epoch [4/100], Step [300/15040], Loss: 0.3872\n",
      "Epoch [4/100], Step [400/15040], Loss: 0.3605\n",
      "Epoch [4/100], Step [500/15040], Loss: 0.3005\n",
      "Epoch [4/100], Step [600/15040], Loss: 0.3395\n",
      "Epoch [4/100], Step [700/15040], Loss: 0.2953\n",
      "Epoch [4/100], Step [800/15040], Loss: 0.2800\n",
      "Epoch [4/100], Step [900/15040], Loss: 0.2113\n",
      "Epoch [4/100], Step [1000/15040], Loss: 0.3760\n",
      "Epoch [4/100], Step [1100/15040], Loss: 0.4507\n",
      "Epoch [4/100], Step [1200/15040], Loss: 0.2895\n",
      "Epoch [4/100], Step [1300/15040], Loss: 0.4379\n",
      "Epoch [4/100], Step [1400/15040], Loss: 0.3165\n",
      "Epoch [4/100], Step [1500/15040], Loss: 0.4194\n",
      "Epoch [4/100], Step [1600/15040], Loss: 0.3488\n",
      "Epoch [4/100], Step [1700/15040], Loss: 0.4168\n",
      "Epoch [4/100], Step [1800/15040], Loss: 0.2483\n",
      "Epoch [4/100], Step [1900/15040], Loss: 0.2568\n",
      "Epoch [4/100], Step [2000/15040], Loss: 0.3639\n",
      "Epoch [4/100], Step [2100/15040], Loss: 0.3967\n",
      "Epoch [4/100], Step [2200/15040], Loss: 0.4039\n",
      "Epoch [4/100], Step [2300/15040], Loss: 0.3964\n",
      "Epoch [4/100], Step [2400/15040], Loss: 0.2682\n",
      "Epoch [4/100], Step [2500/15040], Loss: 0.2371\n",
      "Epoch [4/100], Step [2600/15040], Loss: 0.3599\n",
      "Epoch [4/100], Step [2700/15040], Loss: 0.2870\n",
      "Epoch [4/100], Step [2800/15040], Loss: 0.3794\n",
      "Epoch [4/100], Step [2900/15040], Loss: 0.3553\n",
      "Epoch [4/100], Step [3000/15040], Loss: 0.3679\n",
      "Epoch [4/100], Step [3100/15040], Loss: 0.2675\n",
      "Epoch [4/100], Step [3200/15040], Loss: 0.3372\n",
      "Epoch [4/100], Step [3300/15040], Loss: 0.2936\n",
      "Epoch [4/100], Step [3400/15040], Loss: 0.3121\n",
      "Epoch [4/100], Step [3500/15040], Loss: 0.2605\n",
      "Epoch [4/100], Step [3600/15040], Loss: 0.3119\n",
      "Epoch [4/100], Step [3700/15040], Loss: 0.3615\n",
      "Epoch [4/100], Step [3800/15040], Loss: 0.1808\n",
      "Epoch [4/100], Step [3900/15040], Loss: 0.2448\n",
      "Epoch [4/100], Step [4000/15040], Loss: 0.2565\n",
      "Epoch [4/100], Step [4100/15040], Loss: 0.2150\n",
      "Epoch [4/100], Step [4200/15040], Loss: 0.3185\n",
      "Epoch [4/100], Step [4300/15040], Loss: 0.2525\n",
      "Epoch [4/100], Step [4400/15040], Loss: 0.2963\n",
      "Epoch [4/100], Step [4500/15040], Loss: 0.3016\n",
      "Epoch [4/100], Step [4600/15040], Loss: 0.2492\n",
      "Epoch [4/100], Step [4700/15040], Loss: 0.2424\n",
      "Epoch [4/100], Step [4800/15040], Loss: 0.2165\n",
      "Epoch [4/100], Step [4900/15040], Loss: 0.2162\n",
      "Epoch [4/100], Step [5000/15040], Loss: 0.2929\n",
      "Epoch [4/100], Step [5100/15040], Loss: 0.2089\n",
      "Epoch [4/100], Step [5200/15040], Loss: 0.2356\n",
      "Epoch [4/100], Step [5300/15040], Loss: 0.2242\n",
      "Epoch [4/100], Step [5400/15040], Loss: 0.3402\n",
      "Epoch [4/100], Step [5500/15040], Loss: 0.2818\n",
      "Epoch [4/100], Step [5600/15040], Loss: 0.2225\n",
      "Epoch [4/100], Step [5700/15040], Loss: 0.2833\n",
      "Epoch [4/100], Step [5800/15040], Loss: 0.3026\n",
      "Epoch [4/100], Step [5900/15040], Loss: 0.2113\n",
      "Epoch [4/100], Step [6000/15040], Loss: 0.2967\n",
      "Epoch [4/100], Step [6100/15040], Loss: 0.1882\n",
      "Epoch [4/100], Step [6200/15040], Loss: 0.3417\n",
      "Epoch [4/100], Step [6300/15040], Loss: 0.2420\n",
      "Epoch [4/100], Step [6400/15040], Loss: 0.2308\n",
      "Epoch [4/100], Step [6500/15040], Loss: 0.2446\n",
      "Epoch [4/100], Step [6600/15040], Loss: 0.2836\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch.nn as nn\n",
    "\n",
    "i=1\n",
    "y_pred={}\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "\n",
    "network = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),     \n",
    "    torch.nn.ReLU(),                              \n",
    "    torch.nn.Linear(H, D_out),\n",
    "     )\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for j, (data, labels) in enumerate(loader_train[i]):   # Load a batch of images with its (index, data, class)\n",
    "        optimizer.zero_grad()                             # Intialize the hidden weight to all zeros\n",
    "        y_pred[i] = network(data)                             # Forward pass: compute the output class given a image\n",
    "        loss = criterion(y_pred[i], labels)                 # Compute the loss: difference between the output class and the pre-given label\n",
    "        loss.backward()                                   # Backward pass: compute the weight\n",
    "        optimizer.step()                                  # Optimizer: update the weights of hidden nodes\n",
    "        \n",
    "        if (j+1) % 2000 == 0:                              # Logging\n",
    "            print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n",
    "                 %(epoch+1, n_epochs, j+1, len(tensor_train[i])//batch_size, loss.item()))\n",
    "            \n",
    "        if (j+1) % 10 == 0:                              # keep track of loss value\n",
    "            train_losses.append(loss.item())\n",
    "            train_counter.append(((j+1)*batch_size) + ((epoch)*len(loader_train[i].dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions={}\n",
    "for i in algo:\n",
    "    print(algo_name[i],'\\t',booster[i].eval(test[i]))\n",
    "    predictions[i]=booster[i].predict(test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in algo:\n",
    "# plot all predictions (both signal and background)\n",
    "    plt.figure(figsize=(20,5));\n",
    "\n",
    "    plt.subplot(121);\n",
    "    plt.hist(predictions[i],bins=np.linspace(0,1,50),histtype='step',color='darkgreen',label='All events');\n",
    "# make the plot readable\n",
    "    plt.xlabel('Prediction from BDT',fontsize=12);\n",
    "    plt.ylabel('Events',fontsize=12);\n",
    "    plt.title(algo_name[i])\n",
    "    plt.legend(frameon=False);\n",
    "\n",
    "# plot signal and background separately\n",
    "    plt.subplot(122);\n",
    "    plt.hist(predictions[i][test[i].get_label().astype(bool)],bins=np.linspace(0,1,50),\n",
    "         histtype='step',color='midnightblue',label='signal');\n",
    "    plt.hist(predictions[i][~(test[i].get_label().astype(bool))],bins=np.linspace(0,1,50),\n",
    "         histtype='step',color='firebrick',label='background');\n",
    "# make the plot readable\n",
    "    plt.xlabel('Prediction from BDT',fontsize=12);\n",
    "    plt.ylabel('Events',fontsize=12);\n",
    "    plt.title(algo_name[i])\n",
    "    plt.legend(frameon=False);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,25));\n",
    "for i in algo:\n",
    "# plot all predictions (both signal and background)\n",
    "    \n",
    "    plt.subplot(4,3,i+1)\n",
    "# plot signal and background separately\n",
    "    plt.hist(predictions[i][test[i].get_label().astype(bool)],bins=np.linspace(0,1,50),\n",
    "         histtype='step',color='midnightblue',label='signal');\n",
    "    plt.hist(predictions[i][~(test[i].get_label().astype(bool))],bins=np.linspace(0,1,50),\n",
    "         histtype='step',color='firebrick',label='background');\n",
    "# make the plot readable\n",
    "    plt.xlabel('Prediction from BDT',fontsize=12);\n",
    "    plt.ylabel('Events',fontsize=12);\n",
    "    plt.title(algo_name[i])\n",
    "    plt.legend(frameon=False);\n",
    "plt.savefig(fig_dir+'/predictions.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot importance of features for each algo\n",
    "plt.figure(figsize=(20,30))\n",
    "for i in algo:\n",
    "    ax=plt.subplot(5,2,i+1)\n",
    "    xgb.plot_importance(booster[i],ax,grid=False, title=algo_name[i], importance_type='gain');\n",
    "plt.savefig(fig_dir+'/importance.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build ROC\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "for i in algo:\n",
    "    #buildROC(y_test[i], predictions[i])\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_test[i],predictions[i])\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    plt.plot(tpr,(1-fpr), label ='%s AUC = %0.4f' %(algo_name[i],roc_auc))\n",
    "plt.legend(loc = 'lower left')\n",
    "    #plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim(0.95,1)\n",
    "plt.yscale('log')\n",
    "    #plt.ylim(0.6,1.05)\n",
    "plt.xlabel('efficiency')\n",
    "plt.ylabel('Log Rejection Rate') \n",
    "plt.savefig(fig_dir+'/ROC.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(ytest, ypred, thr):\n",
    "    fpr, tpr, threshold = metrics.roc_curve(ytest,ypred)\n",
    "    roc=pd.DataFrame({'tpr':tpr,'fpr':fpr, 'threshold':threshold})\n",
    "    roc_cut=roc[roc['tpr']>thr];\n",
    "    score=np.min(roc_cut['fpr']);\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thr=0.995\n",
    "scor=[]\n",
    "for i in algo:\n",
    "    scor.append(score(y_test[i], predictions[i],thr))\n",
    "    print('rejection rate for %s at %0.3f threshold=' %(algo_name[i],thr) ,scor[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot background efficiency\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(np.arange(len(algo_name)), scor)\n",
    "plt.xticks(np.arange(len(algo_name)), algo_name);\n",
    "plt.ylabel('Background efficiency')\n",
    "plt.savefig(fig_dir+'/efficiency.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CALCULATE ERROR BARS\n",
    "conf_level=0.99\n",
    "from scipy import stats\n",
    "#normal\n",
    "#def error(total, score, conf_level):\n",
    "#    alpha=(1-conf_level)/2\n",
    "#    sigma=np.sqrt(score*(1-score)/total)\n",
    "#    delta=np.abs(score-stats.norm.ppf(1-alpha,loc=score, scale=sigma))\n",
    "#    return delta\n",
    "\n",
    "#clopper pearson\n",
    "def error(total, score, conf_level):\n",
    "    alpha=(1-conf_level)/2\n",
    "    n=total\n",
    "    k=score*n\n",
    "    lo = score-stats.beta.ppf(alpha/2, k, n-k+1)\n",
    "    hi = stats.beta.ppf(1 - alpha/2, k+1, n-k)-score\n",
    "    return lo, hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comparison with old 2d3d algo\n",
    "thr=0.9\n",
    "#make the comparison\n",
    "b=algo_cut[0].dropna()\n",
    "a=b['cl3d_bdteg']\n",
    "c=np.interp(a, (a.min(), a.max()), (1, 0))\n",
    "\n",
    "score_old=score(b['genpart_pid'],c,thr)\n",
    "print('old bdt score:', score_old)\n",
    "\n",
    "i=0\n",
    "nbins=20\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.hist(c[b['genpart_pid']==0],histtype='step', bins=nbins, label='electron');\n",
    "plt.hist(c[b['genpart_pid']==1],histtype='step',  bins=nbins, label='pion');\n",
    "plt.legend()\n",
    "i=0\n",
    "\n",
    "fpr, tpr, threshold = metrics.roc_curve(b['genpart_pid'],c)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "plt.plot(tpr,np.log(1-fpr), label ='old bdteg AUC = %0.4f' %(roc_auc))\n",
    "\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_test[i],predictions[i])\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "plt.plot(tpr,np.log(1-fpr), label ='%s AUC = %0.4f' %(algo_name[i],roc_auc))\n",
    "    \n",
    "plt.legend(loc = 'lower left')\n",
    "    #plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim(0.8,1)\n",
    "    #plt.ylim(0.6,1.05)\n",
    "plt.xlabel('efficiency')\n",
    "plt.ylabel('Log Rejection Rate')  \n",
    "plt.savefig(fig_dir+'/old.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#plot score\n",
    "%matplotlib inline\n",
    "import matplotlib.ticker as ticker\n",
    "fig=plt.figure(figsize=(15,20))\n",
    "thr=0.95\n",
    "nbins=10\n",
    "conf_level=0.682689492137\n",
    "pred_pt={}\n",
    "y_test_pt={}\n",
    "bins_pt={}\n",
    "score_pt={}\n",
    "y_err_pt={}\n",
    "#faire les bins aprÃ¨s entrainement du bdt\n",
    "#--> predictions[i] vs y_test[i]\n",
    "#predictions[i]=booster[i].predict(test[i])--> binner test[i]\n",
    "# --> binner X_test[i] et y_test[i]\n",
    "for i in algo:\n",
    "    \n",
    "    pt_max=np.max(X_pt[i])\n",
    "    pt_min=np.min(X_pt[i])\n",
    "    range_pt=pt_max-pt_min\n",
    "    bins_pt=[]\n",
    "    score_pt=[]\n",
    "    y_err_pt=[]\n",
    "    y_err_pt_lo=[]\n",
    "    y_err_pt_hi=[]\n",
    "    pred_pt={}\n",
    "    y_test_pt={}\n",
    "    \n",
    "    \n",
    "    X_test[i]['abseta']=np.abs(X_test[i]['cl3d_eta'])\n",
    "    eta_max=np.max(X_test[i]['abseta'])\n",
    "    eta_min=np.min(X_test[i]['abseta'])\n",
    "    range_eta=eta_max-eta_min\n",
    "    bins_eta=[]\n",
    "    score_eta=[]\n",
    "    y_err_eta=[]\n",
    "    y_err_eta_lo=[]\n",
    "    y_err_eta_hi=[]\n",
    "    pred_eta={}\n",
    "    y_test_eta={}\n",
    "    for j in range(nbins):\n",
    "        #make sure last bins are not too small\n",
    "        n_min=X_pt[i].shape[0]/(1.4*nbins)\n",
    "        \n",
    "        sel=(X_pt[i] >= (pt_min+j*range_pt/nbins)) & (X_pt[i] < (pt_min+(j+1)*range_pt/nbins))\n",
    "        \n",
    "        if X_pt[i][sel].shape[0]>n_min:\n",
    "            bins_pt.append(((pt_min+j*range_pt/nbins)+(pt_min+(j+1)*range_pt/nbins))/2)\n",
    "           \n",
    "            pred_pt[j]=predictions[i][sel]\n",
    "            \n",
    "            y_test_pt[j]=y_test[i][sel]\n",
    "        else: \n",
    "            \n",
    "            bins_pt.append(((pt_min+j*range_pt/nbins)+(pt_max))/2)\n",
    "            sel=(X_pt[i] >= (pt_min+j*range_pt/nbins))\n",
    "            pred_pt[j]=predictions[i][sel]\n",
    "            y_test_pt[j]=y_test[i][sel]\n",
    "            \n",
    "            break       \n",
    "\n",
    "    for j in range(len(pred_pt)): \n",
    "        \n",
    "        score_pt.append(score(y_test_pt[j], pred_pt[j],thr))\n",
    "        #print('rejection rate for pt bin %d at %0.3f threshold=' %(j+1,thr),  score_pt[j])\n",
    "        \n",
    "        lo,hi=error(y_test_pt[j].shape[0], score(y_test_pt[j], pred_pt[j], thr), conf_level)\n",
    "        \n",
    "        y_err_pt_lo.append(lo)\n",
    "        y_err_pt_hi.append(hi)\n",
    "        \n",
    "    \n",
    "    for j in range(nbins):\n",
    "        \n",
    "        bins_eta.append(((eta_min+j*range_eta/nbins)+(eta_min+(j+1)*range_eta/nbins))/2)\n",
    "        sel=(X_test[i]['abseta'] > (eta_min+j*range_eta/nbins)) & (X_test[i]['abseta'] < (eta_min+(j+1)*range_eta/nbins))\n",
    "        \n",
    "        pred_eta[j]=predictions[i][sel]\n",
    "        \n",
    "        y_test_eta[j]=y_test[i][sel]\n",
    "        score_eta.append(score(y_test_eta[j], pred_eta[j],thr))\n",
    "        lo, hi=error(y_test_eta[j].shape[0], score(y_test_eta[j], pred_eta[j], thr), conf_level)\n",
    "        y_err_eta_lo.append(lo)\n",
    "        y_err_eta_hi.append(hi)\n",
    "        \n",
    "        \n",
    "        \n",
    "    y_err_pt=[y_err_pt_lo, y_err_pt_hi]\n",
    "    y_err_eta=[y_err_eta_lo, y_err_eta_hi]\n",
    "        \n",
    "    plt.subplot(411);\n",
    "    plt.errorbar(bins_pt, score_pt, y_err_pt, label=algo_name[i]);\n",
    "    plt.ylabel('Background efficiency score');\n",
    "    plt.xlabel('pt');\n",
    "    plt.ylim(-0.001,0.01)\n",
    "    plt.xlim(right=130)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(412);\n",
    "    plt.errorbar(bins_eta, score_eta, y_err_eta, label=algo_name[i]);\n",
    "    plt.ylabel('Background efficiency score');\n",
    "    plt.xlabel('eta');\n",
    "    plt.ylim(-0.001,0.006)\n",
    "    plt.legend()\n",
    "    plt.xlim(right=3.0)\n",
    "    \n",
    "    plt.subplot(413);\n",
    "    plt.errorbar(bins_pt, score_pt, y_err_pt, label=algo_name[i]);\n",
    "    plt.ylabel('Logit Background efficiency score');\n",
    "    plt.xlabel('pt');\n",
    "    plt.xlim(right=130)\n",
    "    plt.yscale('logit')\n",
    "    plt.gca().yaxis.set_minor_locator(ticker.LogLocator(subs=[0]));\n",
    "    plt.ylim(0.000008,0.03)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(414);\n",
    "    plt.errorbar(bins_eta, score_eta, y_err_eta, label=algo_name[i]);\n",
    "    plt.ylabel('Logit Background efficiency score');\n",
    "    plt.xlabel('eta');\n",
    "    plt.yscale('logit')\n",
    "    plt.gca().yaxis.set_minor_locator(ticker.LogLocator(subs=[0]));\n",
    "    plt.ylim(0.000008,0.03)\n",
    "    plt.xlim(right=3.0)\n",
    "    plt.legend()\n",
    "    \n",
    "   \n",
    "    \n",
    "    plt.suptitle('Background efficiency at %0.3f signal efficiency at %0.2f CL for different algorithms' %(thr, conf_level), fontsize=14)\n",
    "    plt.savefig(fig_dir+'/Algo_comparison.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'if __name__==\\'__main__\\':\\n    parser = optparse.OptionParser()\\n    parser.add_option(\"-t\", \"--threshold\", dest=\"thr\",\\n                  help=\"set the threshold for deltar\", default=\"0.05\")\\n    parser.add_option(\"-p\",\"--path\", dest=\"path\", help=\"select the path to data\")\\n    parser.add_option(\"-b\", \"--batch\", dest=\"files\", help=\"select the files to process\")\\n    parser.add_option(\"-s\", \"--save\", dest=\"savedir\", help=\"where to save the processed files\")\\n    (opt, args) = parser.parse_args()\\n    \\n    path=opt.path\\n    thr=opt.thr\\n    files=opt.files\\n    savedir=opt.savedir\\n    \\n    preprocessing(path, files, savedir, thr)'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import uproot\n",
    "from datetime import date\n",
    "import optparse\n",
    "\n",
    "\n",
    "\n",
    "algo_name=['T23', 'S', 'S0', 'S10', 'S20', 'T', 'T0', 'T10', 'T20']\n",
    "\n",
    "\n",
    "def deltar(df):\n",
    "    df['deta']=df['cl3d_eta']-df['genpart_exeta']\n",
    "    df['dphi']=np.abs(df['cl3d_phi']-df['genpart_exphi'])\n",
    "    sel=df['dphi']>np.pi\n",
    "    df['dphi']-=sel*(2*np.pi)\n",
    "    return(np.sqrt(df['dphi']*df['dphi']+df['deta']*df['deta']))\n",
    "    \n",
    "def matching(event):\n",
    "    return event.cl3d_pt==event.cl3d_pt.max()\n",
    "\n",
    "def openroot(path, files):\n",
    "    os.chdir(path)\n",
    "    algo={}\n",
    "    branches_gen=['event','genpart_pid','genpart_exphi', 'genpart_exeta','genpart_gen','genpart_reachedEE', 'genpart_pt', 'genpart_energy','genpart_fbrem']\n",
    "    branches_cl3d=['event','cl3d_pt','cl3d_eta','cl3d_phi','cl3d_showerlength','cl3d_coreshowerlength','cl3d_firstlayer','cl3d_maxlayer','cl3d_seetot','cl3d_spptot','cl3d_szz', 'cl3d_srrtot', 'cl3d_srrmean']\n",
    "    branches_T23=branches_cl3d+['cl3d_bdteg','cl3d_quality']\n",
    "    \n",
    "    for i,filename in enumerate(files,1):\n",
    "        if i==1:\n",
    "            gen=uproot.open(filename)['Floatingpoint8ThresholdRef2dRef3dGenclustersntuple;1/HGCalTriggerNtuple'].pandas.df(branches_gen,flatten=True)\n",
    "            algo[0]=uproot.open(filename)['Floatingpoint8ThresholdRef2dRef3dGenclustersntuple;1/HGCalTriggerNtuple'].pandas.df(branches_T23,flatten=True)\n",
    "            algo[1]=uproot.open(filename)['Floatingpoint8SupertriggercellDummyHistomaxClustersntuple;1/HGCalTriggerNtuple'].pandas.df(branches_cl3d,flatten=True)\n",
    "            algo[2]=uproot.open(filename)['Floatingpoint8SupertriggercellDummyHistomaxvardrth0Clustersntuple;1/HGCalTriggerNtuple'].pandas.df(branches_cl3d,flatten=True)\n",
    "            algo[3]=uproot.open(filename)['Floatingpoint8SupertriggercellDummyHistomaxvardrth10Clustersntuple;1/HGCalTriggerNtuple'].pandas.df(branches_cl3d,flatten=True)\n",
    "            algo[4]=uproot.open(filename)['Floatingpoint8SupertriggercellDummyHistomaxvardrth20Clustersntuple;1/HGCalTriggerNtuple'].pandas.df(branches_cl3d,flatten=True)\n",
    "            algo[5]=uproot.open(filename)['Floatingpoint8ThresholdDummyHistomaxClustersntuple;1/HGCalTriggerNtuple'].pandas.df(branches_cl3d,flatten=True)\n",
    "            algo[6]=uproot.open(filename)['Floatingpoint8ThresholdDummyHistomaxvardrth0Clustersntuple;1/HGCalTriggerNtuple'].pandas.df(branches_cl3d,flatten=True)\n",
    "            algo[7]=uproot.open(filename)['Floatingpoint8ThresholdDummyHistomaxvardrth10Clustersntuple;1/HGCalTriggerNtuple'].pandas.df(branches_cl3d,flatten=True)\n",
    "            algo[8]=uproot.open(filename)['Floatingpoint8ThresholdDummyHistomaxvardrth20Clustersntuple;1/HGCalTriggerNtuple'].pandas.df(branches_cl3d,flatten=True)\n",
    "        \n",
    "        else:\n",
    "            gen=pd.concat([gen,uproot.open(filename)['Floatingpoint8ThresholdRef2dRef3dGenclustersntuple;1/HGCalTriggerNtuple'].pandas.df(branches_gen,flatten=True)])\n",
    "            algo[0]=pd.concat([algo[0],uproot.open(filename)['Floatingpoint8ThresholdRef2dRef3dGenclustersntuple;1/HGCalTriggerNtuple'].pandas.df(branches_T23,flatten=True)])\n",
    "            algo[1]=pd.concat([algo[1],uproot.open(filename)['Floatingpoint8SupertriggercellDummyHistomaxClustersntuple;1/HGCalTriggerNtuple'].pandas.df(branches_cl3d,flatten=True)])\n",
    "            algo[2]=pd.concat([algo[2],uproot.open(filename)['Floatingpoint8SupertriggercellDummyHistomaxvardrth0Clustersntuple;1/HGCalTriggerNtuple'].pandas.df(branches_cl3d,flatten=True)])\n",
    "            algo[3]=pd.concat([algo[3],uproot.open(filename)['Floatingpoint8SupertriggercellDummyHistomaxvardrth10Clustersntuple;1/HGCalTriggerNtuple'].pandas.df(branches_cl3d,flatten=True)])\n",
    "            algo[4]=pd.concat([algo[4],uproot.open(filename)['Floatingpoint8SupertriggercellDummyHistomaxvardrth20Clustersntuple;1/HGCalTriggerNtuple'].pandas.df(branches_cl3d,flatten=True)])\n",
    "            algo[5]=pd.concat([algo[5],uproot.open(filename)['Floatingpoint8ThresholdDummyHistomaxClustersntuple;1/HGCalTriggerNtuple'].pandas.df(branches_cl3d,flatten=True)])\n",
    "            algo[6]=pd.concat([algo[6],uproot.open(filename)['Floatingpoint8ThresholdDummyHistomaxvardrth0Clustersntuple;1/HGCalTriggerNtuple'].pandas.df(branches_cl3d,flatten=True)])\n",
    "            algo[7]=pd.concat([algo[7],uproot.open(filename)['Floatingpoint8ThresholdDummyHistomaxvardrth10Clustersntuple;1/HGCalTriggerNtuple'].pandas.df(branches_cl3d,flatten=True)])\n",
    "            algo[8]=pd.concat([algo[8],uproot.open(filename)['Floatingpoint8ThresholdDummyHistomaxvardrth20Clustersntuple;1/HGCalTriggerNtuple'].pandas.df(branches_cl3d,flatten=True)])\n",
    "        \n",
    "    return(gen, algo)\n",
    "\n",
    "def preprocessing(path, files, savedir,  thr):\n",
    "    \n",
    "    gen,algo=openroot(path, files)\n",
    "    n_rec={}\n",
    "    algo_clean={}\n",
    "    \n",
    "    \n",
    "    \n",
    "    #clean gen from particles that are not the originals or didn't reach endcap\n",
    "    sel=gen['genpart_reachedEE']==2 \n",
    "    gen_clean=gen[sel]\n",
    "    sel=gen_clean['genpart_gen']!=-1\n",
    "    gen_clean=gen_clean[sel]\n",
    "\n",
    "    #split df_gen_clean in two by eta sign\n",
    "    sel1=gen_clean['genpart_exeta']<=0\n",
    "    sel2=gen_clean['genpart_exeta']>0\n",
    "    gen_neg=gen_clean[sel1]\n",
    "    gen_pos=gen_clean[sel2]\n",
    "\n",
    "    gen_pos.set_index('event', inplace=True)\n",
    "    gen_neg.set_index('event', inplace=True)\n",
    "    \n",
    "    for i in algo:\n",
    "        #split clusters in two by eta sign\n",
    "        sel1=algo[i]['cl3d_eta']<=0\n",
    "        sel2=algo[i]['cl3d_eta']>0\n",
    "        algo_neg=algo[i][sel1]\n",
    "        algo_pos=algo[i][sel2]\n",
    "        #set the indices\n",
    "        algo_pos.set_index('event', inplace=True)\n",
    "        algo_neg.set_index('event', inplace=True)\n",
    "        #merging\n",
    "        algo_pos_merged=gen_pos.join(algo_pos, how='left', rsuffix='_algo')\n",
    "        algo_neg_merged=gen_neg.join(algo_neg, how='left', rsuffix='_algo')\n",
    "        #calculate deltar\n",
    "        algo_pos_merged['deltar']=deltar(algo_pos_merged)\n",
    "        algo_neg_merged['deltar']=deltar(algo_neg_merged)\n",
    "        \n",
    "        #keep the unreconstructed values (NaN)\n",
    "        sel=pd.isna(algo_pos_merged['deltar']) \n",
    "        unmatched_pos=algo_pos_merged[sel]\n",
    "        sel=pd.isna(algo_neg_merged['deltar'])  \n",
    "        unmatched_neg=algo_neg_merged[sel]\n",
    "        unmatched_pos['matches']=False\n",
    "        unmatched_neg['matches']=False\n",
    "        \n",
    "        #select deltar under thr\n",
    "        sel=algo_pos_merged['deltar']<=thr\n",
    "        algo_pos_merged=algo_pos_merged[sel]\n",
    "        sel=algo_neg_merged['deltar']<=thr\n",
    "        algo_neg_merged=algo_neg_merged[sel]\n",
    "        \n",
    "        #matching\n",
    "        group=algo_pos_merged.groupby('event')\n",
    "        n_rec_pos=group['cl3d_pt'].size()\n",
    "        algo_pos_merged['best_match']=group.apply(matching).array\n",
    "        group=algo_neg_merged.groupby('event')\n",
    "        n_rec_neg=group['cl3d_pt'].size()\n",
    "        algo_neg_merged['best_match']=group.apply(matching).array\n",
    "        \n",
    "        #keep only matched clusters \n",
    "        sel=algo_pos_merged['best_match']==True\n",
    "        algo_pos_merged=algo_pos_merged[sel]\n",
    "    \n",
    "        sel=algo_neg_merged['best_match']==True\n",
    "        algo_neg_merged=algo_neg_merged[sel]\n",
    "    \n",
    "        #remerge with NaN values\n",
    "        algo_pos_merged=pd.concat([algo_pos_merged, unmatched_pos], sort=False).sort_values('event') \n",
    "        algo_neg_merged=pd.concat([algo_neg_merged, unmatched_neg], sort=False).sort_values('event')\n",
    "        \n",
    "        n_rec[i]=n_rec_pos.append(n_rec_neg)\n",
    "        algo_clean[i]=pd.concat([algo_neg_merged,algo_pos_merged], sort=False).sort_values('event')\n",
    "\n",
    "        algo_clean[i]['matches']=algo_clean[i]['matches'].replace(np.nan, True)\n",
    "        algo_clean[i].drop(columns=['best_match'], inplace=True)\n",
    "\n",
    "        #save files to savedir\n",
    "    os.chdir(savedir)   \n",
    "    gen_clean.to_csv('gen_clean.csv')\n",
    "    for i in algo:\n",
    "        algo_clean[i].to_csv('{}.csv'.format(algo_name[i]))\n",
    "        \n",
    "        \n",
    "if __name__=='__main__':\n",
    "    parser = optparse.OptionParser()\n",
    "    parser.add_option(\"-t\", \"--threshold\", dest=\"thr\",\n",
    "                  help=\"set the threshold for deltar\", default=\"0.05\")\n",
    "    parser.add_option(\"-p\",\"--path\", dest=\"path\", help=\"select the path to data\")\n",
    "    parser.add_option(\"-b\", \"--batch\", dest=\"files\", help=\"select the files to process\")\n",
    "    parser.add_option(\"-s\", \"--save\", dest=\"savedir\", help=\"where to save the processed files\")\n",
    "    (opt, args) = parser.parse_args()\n",
    "    \n",
    "    path=opt.path\n",
    "    thr=opt.thr\n",
    "    files=opt.files\n",
    "    savedir=opt.savedir\n",
    "    \n",
    "    preprocessing(path, files, savedir, thr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

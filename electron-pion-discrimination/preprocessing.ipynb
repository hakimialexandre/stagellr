{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import uproot\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory=os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_elec='./SingleE_FlatPt-2to100/EleGunPt2to100_PU0_L1TFall17_multialgo/190226_144458'\n",
    "os.chdir(path_elec)\n",
    "#select interesting branches\n",
    "branches_gen=['event','genpart_pid','genpart_exphi', 'genpart_exeta','genpart_gen','genpart_reachedEE', 'genpart_pt', 'genpart_energy','genpart_fbrem']\n",
    "branches_cl3d=['event','cl3d_pt','cl3d_eta','cl3d_phi','cl3d_showerlength','cl3d_coreshowerlength','cl3d_firstlayer','cl3d_maxlayer','cl3d_seetot','cl3d_spptot','cl3d_szz', 'cl3d_srrtot', 'cl3d_srrmean']\n",
    "branches_T23=branches_cl3d+['cl3d_bdteg','cl3d_quality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:117: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\ProgramData\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:118: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 29min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for filename in os.listdir(os.getcwd()):\n",
    "    if filename=='ntuple_1.root':\n",
    "        gen=uproot.open(filename)['Floatingpoint8ThresholdRef2dRef3dGenclustersntuple;1/HGCalTriggerNtuple'].pandas.df(branches_gen,flatten=True)\n",
    "        df_T23=uproot.open(filename)['Floatingpoint8ThresholdRef2dRef3dGenclustersntuple;1/HGCalTriggerNtuple'].pandas.df(branches_T23,flatten=True)\n",
    "        df_SDH=uproot.open(filename)['Floatingpoint8SupertriggercellDummyHistomaxClustersntuple;1/HGCalTriggerNtuple'].pandas.df(branches_cl3d,flatten=True)\n",
    "        df_SDH0=uproot.open(filename)['Floatingpoint8SupertriggercellDummyHistomaxvardrth0Clustersntuple;1/HGCalTriggerNtuple'].pandas.df(branches_cl3d,flatten=True)\n",
    "        df_SDH10=uproot.open(filename)['Floatingpoint8SupertriggercellDummyHistomaxvardrth10Clustersntuple;1/HGCalTriggerNtuple'].pandas.df(branches_cl3d,flatten=True)\n",
    "        df_SDH20=uproot.open(filename)['Floatingpoint8SupertriggercellDummyHistomaxvardrth20Clustersntuple;1/HGCalTriggerNtuple'].pandas.df(branches_cl3d,flatten=True)\n",
    "        df_TDH=uproot.open(filename)['Floatingpoint8ThresholdDummyHistomaxClustersntuple;1/HGCalTriggerNtuple'].pandas.df(branches_cl3d,flatten=True)\n",
    "        df_TDH0=uproot.open(filename)['Floatingpoint8ThresholdDummyHistomaxvardrth0Clustersntuple;1/HGCalTriggerNtuple'].pandas.df(branches_cl3d,flatten=True)\n",
    "        df_TDH10=uproot.open(filename)['Floatingpoint8ThresholdDummyHistomaxvardrth10Clustersntuple;1/HGCalTriggerNtuple'].pandas.df(branches_cl3d,flatten=True)\n",
    "        df_TDH20=uproot.open(filename)['Floatingpoint8ThresholdDummyHistomaxvardrth20Clustersntuple;1/HGCalTriggerNtuple'].pandas.df(branches_cl3d,flatten=True)\n",
    "    else:\n",
    "        Gen_temp=uproot.open(filename)['Floatingpoint8ThresholdRef2dRef3dGenclustersntuple;1/HGCalTriggerNtuple'].pandas.df(branches_gen,flatten=True)\n",
    "        T23_temp=uproot.open(filename)['Floatingpoint8ThresholdRef2dRef3dGenclustersntuple;1/HGCalTriggerNtuple'].pandas.df(branches_T23,flatten=True)\n",
    "        SDH_temp=uproot.open(filename)['Floatingpoint8SupertriggercellDummyHistomaxClustersntuple;1/HGCalTriggerNtuple'].pandas.df(branches_cl3d,flatten=True)\n",
    "        SDH0_temp=uproot.open(filename)['Floatingpoint8SupertriggercellDummyHistomaxvardrth0Clustersntuple;1/HGCalTriggerNtuple'].pandas.df(branches_cl3d,flatten=True)\n",
    "        SDH10_temp=uproot.open(filename)['Floatingpoint8SupertriggercellDummyHistomaxvardrth10Clustersntuple;1/HGCalTriggerNtuple'].pandas.df(branches_cl3d,flatten=True)\n",
    "        SDH20_temp=uproot.open(filename)['Floatingpoint8SupertriggercellDummyHistomaxvardrth20Clustersntuple;1/HGCalTriggerNtuple'].pandas.df(branches_cl3d,flatten=True)\n",
    "        TDH_temp=uproot.open(filename)['Floatingpoint8ThresholdDummyHistomaxClustersntuple;1/HGCalTriggerNtuple'].pandas.df(branches_cl3d,flatten=True)\n",
    "        TDH0_temp=uproot.open(filename)['Floatingpoint8ThresholdDummyHistomaxvardrth0Clustersntuple;1/HGCalTriggerNtuple'].pandas.df(branches_cl3d,flatten=True)\n",
    "        TDH10_temp=uproot.open(filename)['Floatingpoint8ThresholdDummyHistomaxvardrth10Clustersntuple;1/HGCalTriggerNtuple'].pandas.df(branches_cl3d,flatten=True)\n",
    "        TDH20_temp=uproot.open(filename)['Floatingpoint8ThresholdDummyHistomaxvardrth20Clustersntuple;1/HGCalTriggerNtuple'].pandas.df(branches_cl3d,flatten=True)\n",
    "        \n",
    "        gen=pd.concat([gen, Gen_temp])\n",
    "        df_T23=pd.concat([df_T23, T23_temp])\n",
    "        df_SDH=pd.concat([df_SDH, SDH_temp])\n",
    "        df_SDH0=pd.concat([df_SDH0, SDH0_temp])\n",
    "        df_SDH10=pd.concat([df_SDH10, SDH10_temp])\n",
    "        df_SDH20=pd.concat([df_SDH20, SDH20_temp])\n",
    "        df_TDH=pd.concat([df_TDH, TDH_temp])\n",
    "        df_TDH0=pd.concat([df_TDH0, TDH0_temp])\n",
    "        df_TDH10=pd.concat([df_TDH10, TDH10_temp])\n",
    "        df_TDH20=pd.concat([df_TDH20, TDH20_temp])\n",
    "        \n",
    "algo={}\n",
    "algo[0]=df_T23\n",
    "algo[1]=df_SDH\n",
    "algo[2]=df_SDH0\n",
    "algo[3]=df_SDH10\n",
    "algo[4]=df_SDH20\n",
    "algo[5]=df_TDH\n",
    "algo[6]=df_TDH0\n",
    "algo[7]=df_TDH10\n",
    "algo[8]=df_TDH20\n",
    "\n",
    "algo_clean={}\n",
    "algo_name={}\n",
    "algo_name[0]='T23'\n",
    "algo_name[1]='SDH'\n",
    "algo_name[2]='SDH0'\n",
    "algo_name[3]='SDH10'\n",
    "algo_name[4]='SDH20'\n",
    "algo_name[5]='TDH'\n",
    "algo_name[6]='TDH0'\n",
    "algo_name[7]='TDH10'\n",
    "algo_name[8]='TDH20'\n",
    "\n",
    "#clean gen from particles that are not the originals or didn't reach endcap\n",
    "sel=gen['genpart_reachedEE']==2 \n",
    "gen_clean=gen[sel]\n",
    "sel=gen_clean['genpart_gen']!=-1\n",
    "gen_clean=gen_clean[sel]\n",
    "\n",
    "#split df_gen_clean in two by eta sign\n",
    "sel1=gen_clean['genpart_exeta']<=0\n",
    "sel2=gen_clean['genpart_exeta']>0\n",
    "gen_neg=gen_clean[sel1]\n",
    "gen_pos=gen_clean[sel2]\n",
    "\n",
    "gen_pos.set_index('event', inplace=True)\n",
    "gen_neg.set_index('event', inplace=True)\n",
    "thr=0.05\n",
    "def matching(event):\n",
    "    return event.cl3d_pt==event.cl3d_pt.max()\n",
    "\n",
    "\n",
    "n_rec={}\n",
    "deltar={}\n",
    "#preprocessing for every algo\n",
    "for i in algo:\n",
    "    #split clusters in two by eta sign\n",
    "    sel1=algo[i]['cl3d_eta']<=0\n",
    "    sel2=algo[i]['cl3d_eta']>0\n",
    "    algo_neg=algo[i][sel1]\n",
    "    algo_pos=algo[i][sel2]\n",
    "    #set the indices\n",
    "    algo_pos.set_index('event', inplace=True)\n",
    "    algo_neg.set_index('event', inplace=True)\n",
    "    #merging\n",
    "    algo_pos_merged=gen_pos.join(algo_pos, how='left', rsuffix='_algo')\n",
    "    algo_neg_merged=gen_neg.join(algo_neg, how='left', rsuffix='_algo')\n",
    "    #calculate deta and phi\n",
    "    algo_pos_merged['deta']=algo_pos_merged['cl3d_eta']-algo_pos_merged['genpart_exeta']\n",
    "    algo_pos_merged['dphi']=np.abs(algo_pos_merged['cl3d_phi']-algo_pos_merged['genpart_exphi'])\n",
    "\n",
    "    algo_neg_merged['deta']=algo_neg_merged['cl3d_eta']-algo_neg_merged['genpart_exeta']\n",
    "    algo_neg_merged['dphi']=np.abs(algo_neg_merged['cl3d_phi']-algo_neg_merged['genpart_exphi'])\n",
    "    \n",
    "    #get dphi between -pi and pi\n",
    "    sel=algo_pos_merged['dphi']>np.pi\n",
    "    algo_pos_merged['dphi']-=sel*(2*np.pi)\n",
    "    algo_pos_merged['deltar']=np.sqrt(algo_pos_merged['dphi']*algo_pos_merged['dphi']+algo_pos_merged['deta']*algo_pos_merged['deta'])\n",
    "    deltar_pos=algo_pos_merged['deltar']\n",
    "    \n",
    "    #get dphi between -pi and pi\n",
    "    sel=algo_neg_merged['dphi']>np.pi\n",
    "    algo_neg_merged['dphi']-=sel*(2*np.pi)\n",
    "    algo_neg_merged['deltar']=np.sqrt(algo_neg_merged['dphi']*algo_neg_merged['dphi']+algo_neg_merged['deta']*algo_neg_merged['deta'])\n",
    "    deltar_neg=algo_neg_merged['deltar']\n",
    "    \n",
    "    #keep the unreconstructed values (NaN)\n",
    "    sel=pd.isna(algo_pos_merged['deltar']) \n",
    "    unmatched_pos=algo_pos_merged[sel]\n",
    "    sel=pd.isna(algo_neg_merged['deltar'])  \n",
    "    unmatched_neg=algo_neg_merged[sel]\n",
    "    unmatched_pos['matches']=False\n",
    "    unmatched_neg['matches']=False\n",
    "    \n",
    "    \n",
    "    #select deltar under thr\n",
    "    sel=algo_pos_merged['deltar']<=thr\n",
    "    algo_pos_merged=algo_pos_merged[sel]\n",
    "    sel=algo_neg_merged['deltar']<=thr\n",
    "    algo_neg_merged=algo_neg_merged[sel]\n",
    "    \n",
    "    #matching\n",
    "    group=algo_pos_merged.groupby('event')\n",
    "    n_rec_pos=group['cl3d_pt'].size()\n",
    "    algo_pos_merged['best_match']=group.apply(matching).array\n",
    "    group=algo_neg_merged.groupby('event')\n",
    "    n_rec_neg=group['cl3d_pt'].size()\n",
    "    algo_neg_merged['best_match']=group.apply(matching).array\n",
    "    #keep only matched clusters \n",
    "    sel=algo_pos_merged['best_match']==True\n",
    "    algo_pos_merged=algo_pos_merged[sel]\n",
    "    \n",
    "    sel=algo_neg_merged['best_match']==True\n",
    "    algo_neg_merged=algo_neg_merged[sel]\n",
    "    \n",
    "    #remerge with NaN values\n",
    "    algo_pos_merged=pd.concat([algo_pos_merged, unmatched_pos], sort=False).sort_values('event') \n",
    "    algo_neg_merged=pd.concat([algo_neg_merged, unmatched_neg], sort=False).sort_values('event')\n",
    "\n",
    "    \n",
    "    \n",
    "    deltar[i]=deltar_pos.append(deltar_neg)\n",
    "    n_rec[i]=n_rec_pos.append(n_rec_neg)\n",
    "    algo_clean[i]=pd.concat([algo_neg_merged,algo_pos_merged], sort=False).sort_values('event')\n",
    "\n",
    "    algo_clean[i]['matches']=algo_clean[i]['matches'].replace(np.nan, True)\n",
    "    algo_clean[i].drop(columns=['best_match'], inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.chdir(directory)\n",
    "os.chdir('./electron')\n",
    "gen_clean.to_csv('gen_clean.csv')\n",
    "algo_clean[0].to_csv('T23.csv')\n",
    "algo_clean[1].to_csv('SDH.csv')\n",
    "algo_clean[2].to_csv('SDH0.csv')\n",
    "algo_clean[3].to_csv('SDH10.csv')\n",
    "algo_clean[4].to_csv('SDH20.csv')\n",
    "algo_clean[5].to_csv('TDH.csv')\n",
    "algo_clean[6].to_csv('TDH0.csv')\n",
    "algo_clean[7].to_csv('TDH10.csv')\n",
    "algo_clean[8].to_csv('TDH20.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(directory)\n",
    "path_pions='./SinglePion_FlatPt-2to100/PionGunPt2to100_PU0_L1TFall17_multialgo/190226_145137'\n",
    "os.chdir(path_pions)\n",
    "#select interesting branches\n",
    "branches_gen=['event','genpart_pid','genpart_exphi', 'genpart_exeta','genpart_gen','genpart_reachedEE', 'genpart_pt', 'genpart_energy','genpart_fbrem']\n",
    "branches_cl3d=['event','cl3d_pt','cl3d_eta','cl3d_phi','cl3d_showerlength','cl3d_coreshowerlength','cl3d_firstlayer','cl3d_maxlayer','cl3d_seetot','cl3d_spptot','cl3d_szz', 'cl3d_srrtot', 'cl3d_srrmean']\n",
    "branches_T23=branches_cl3d+['cl3d_bdteg','cl3d_quality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:117: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\ProgramData\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:118: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 29min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for filename in os.listdir(os.getcwd()):\n",
    "    if filename=='ntuple_1.root':\n",
    "        gen=uproot.open(filename)['Floatingpoint8ThresholdRef2dRef3dGenclustersntuple;1/HGCalTriggerNtuple'].pandas.df(branches_gen,flatten=True)\n",
    "        df_T23=uproot.open(filename)['Floatingpoint8ThresholdRef2dRef3dGenclustersntuple;1/HGCalTriggerNtuple'].pandas.df(branches_T23,flatten=True)\n",
    "        df_SDH=uproot.open(filename)['Floatingpoint8SupertriggercellDummyHistomaxClustersntuple;1/HGCalTriggerNtuple'].pandas.df(branches_cl3d,flatten=True)\n",
    "        df_SDH0=uproot.open(filename)['Floatingpoint8SupertriggercellDummyHistomaxvardrth0Clustersntuple;1/HGCalTriggerNtuple'].pandas.df(branches_cl3d,flatten=True)\n",
    "        df_SDH10=uproot.open(filename)['Floatingpoint8SupertriggercellDummyHistomaxvardrth10Clustersntuple;1/HGCalTriggerNtuple'].pandas.df(branches_cl3d,flatten=True)\n",
    "        df_SDH20=uproot.open(filename)['Floatingpoint8SupertriggercellDummyHistomaxvardrth20Clustersntuple;1/HGCalTriggerNtuple'].pandas.df(branches_cl3d,flatten=True)\n",
    "        df_TDH=uproot.open(filename)['Floatingpoint8ThresholdDummyHistomaxClustersntuple;1/HGCalTriggerNtuple'].pandas.df(branches_cl3d,flatten=True)\n",
    "        df_TDH0=uproot.open(filename)['Floatingpoint8ThresholdDummyHistomaxvardrth0Clustersntuple;1/HGCalTriggerNtuple'].pandas.df(branches_cl3d,flatten=True)\n",
    "        df_TDH10=uproot.open(filename)['Floatingpoint8ThresholdDummyHistomaxvardrth10Clustersntuple;1/HGCalTriggerNtuple'].pandas.df(branches_cl3d,flatten=True)\n",
    "        df_TDH20=uproot.open(filename)['Floatingpoint8ThresholdDummyHistomaxvardrth20Clustersntuple;1/HGCalTriggerNtuple'].pandas.df(branches_cl3d,flatten=True)\n",
    "    else:\n",
    "        Gen_temp=uproot.open(filename)['Floatingpoint8ThresholdRef2dRef3dGenclustersntuple;1/HGCalTriggerNtuple'].pandas.df(branches_gen,flatten=True)\n",
    "        T23_temp=uproot.open(filename)['Floatingpoint8ThresholdRef2dRef3dGenclustersntuple;1/HGCalTriggerNtuple'].pandas.df(branches_T23,flatten=True)\n",
    "        SDH_temp=uproot.open(filename)['Floatingpoint8SupertriggercellDummyHistomaxClustersntuple;1/HGCalTriggerNtuple'].pandas.df(branches_cl3d,flatten=True)\n",
    "        SDH0_temp=uproot.open(filename)['Floatingpoint8SupertriggercellDummyHistomaxvardrth0Clustersntuple;1/HGCalTriggerNtuple'].pandas.df(branches_cl3d,flatten=True)\n",
    "        SDH10_temp=uproot.open(filename)['Floatingpoint8SupertriggercellDummyHistomaxvardrth10Clustersntuple;1/HGCalTriggerNtuple'].pandas.df(branches_cl3d,flatten=True)\n",
    "        SDH20_temp=uproot.open(filename)['Floatingpoint8SupertriggercellDummyHistomaxvardrth20Clustersntuple;1/HGCalTriggerNtuple'].pandas.df(branches_cl3d,flatten=True)\n",
    "        TDH_temp=uproot.open(filename)['Floatingpoint8ThresholdDummyHistomaxClustersntuple;1/HGCalTriggerNtuple'].pandas.df(branches_cl3d,flatten=True)\n",
    "        TDH0_temp=uproot.open(filename)['Floatingpoint8ThresholdDummyHistomaxvardrth0Clustersntuple;1/HGCalTriggerNtuple'].pandas.df(branches_cl3d,flatten=True)\n",
    "        TDH10_temp=uproot.open(filename)['Floatingpoint8ThresholdDummyHistomaxvardrth10Clustersntuple;1/HGCalTriggerNtuple'].pandas.df(branches_cl3d,flatten=True)\n",
    "        TDH20_temp=uproot.open(filename)['Floatingpoint8ThresholdDummyHistomaxvardrth20Clustersntuple;1/HGCalTriggerNtuple'].pandas.df(branches_cl3d,flatten=True)\n",
    "        \n",
    "        gen=pd.concat([gen, Gen_temp])\n",
    "        df_T23=pd.concat([df_T23, T23_temp])\n",
    "        df_SDH=pd.concat([df_SDH, SDH_temp])\n",
    "        df_SDH0=pd.concat([df_SDH0, SDH0_temp])\n",
    "        df_SDH10=pd.concat([df_SDH10, SDH10_temp])\n",
    "        df_SDH20=pd.concat([df_SDH20, SDH20_temp])\n",
    "        df_TDH=pd.concat([df_TDH, TDH_temp])\n",
    "        df_TDH0=pd.concat([df_TDH0, TDH0_temp])\n",
    "        df_TDH10=pd.concat([df_TDH10, TDH10_temp])\n",
    "        df_TDH20=pd.concat([df_TDH20, TDH20_temp])\n",
    "        \n",
    "algo={}\n",
    "algo[0]=df_T23\n",
    "algo[1]=df_SDH\n",
    "algo[2]=df_SDH0\n",
    "algo[3]=df_SDH10\n",
    "algo[4]=df_SDH20\n",
    "algo[5]=df_TDH\n",
    "algo[6]=df_TDH0\n",
    "algo[7]=df_TDH10\n",
    "algo[8]=df_TDH20\n",
    "\n",
    "algo_clean={}\n",
    "algo_name={}\n",
    "algo_name[0]='T23'\n",
    "algo_name[1]='SDH'\n",
    "algo_name[2]='SDH0'\n",
    "algo_name[3]='SDH10'\n",
    "algo_name[4]='SDH20'\n",
    "algo_name[5]='TDH'\n",
    "algo_name[6]='TDH0'\n",
    "algo_name[7]='TDH10'\n",
    "algo_name[8]='TDH20'\n",
    "\n",
    "#clean gen from particles that are not the originals or didn't reach endcap\n",
    "sel=gen['genpart_reachedEE']==2 \n",
    "gen_clean=gen[sel]\n",
    "sel=gen_clean['genpart_gen']!=-1\n",
    "gen_clean=gen_clean[sel]\n",
    "\n",
    "#split df_gen_clean in two by eta sign\n",
    "sel1=gen_clean['genpart_exeta']<=0\n",
    "sel2=gen_clean['genpart_exeta']>0\n",
    "gen_neg=gen_clean[sel1]\n",
    "gen_pos=gen_clean[sel2]\n",
    "\n",
    "gen_pos.set_index('event', inplace=True)\n",
    "gen_neg.set_index('event', inplace=True)\n",
    "thr=0.05\n",
    "def matching(event):\n",
    "    return event.cl3d_pt==event.cl3d_pt.max()\n",
    "\n",
    "\n",
    "n_rec={}\n",
    "deltar={}\n",
    "#preprocessing for every algo\n",
    "for i in algo:\n",
    "    #split clusters in two by eta sign\n",
    "    sel1=algo[i]['cl3d_eta']<=0\n",
    "    sel2=algo[i]['cl3d_eta']>0\n",
    "    algo_neg=algo[i][sel1]\n",
    "    algo_pos=algo[i][sel2]\n",
    "    #set the indices\n",
    "    algo_pos.set_index('event', inplace=True)\n",
    "    algo_neg.set_index('event', inplace=True)\n",
    "    #merging\n",
    "    algo_pos_merged=gen_pos.join(algo_pos, how='left', rsuffix='_algo')\n",
    "    algo_neg_merged=gen_neg.join(algo_neg, how='left', rsuffix='_algo')\n",
    "    #calculate deta and phi\n",
    "    algo_pos_merged['deta']=algo_pos_merged['cl3d_eta']-algo_pos_merged['genpart_exeta']\n",
    "    algo_pos_merged['dphi']=np.abs(algo_pos_merged['cl3d_phi']-algo_pos_merged['genpart_exphi'])\n",
    "\n",
    "    algo_neg_merged['deta']=algo_neg_merged['cl3d_eta']-algo_neg_merged['genpart_exeta']\n",
    "    algo_neg_merged['dphi']=np.abs(algo_neg_merged['cl3d_phi']-algo_neg_merged['genpart_exphi'])\n",
    "    \n",
    "    #get dphi between -pi and pi\n",
    "    sel=algo_pos_merged['dphi']>np.pi\n",
    "    algo_pos_merged['dphi']-=sel*(2*np.pi)\n",
    "    algo_pos_merged['deltar']=np.sqrt(algo_pos_merged['dphi']*algo_pos_merged['dphi']+algo_pos_merged['deta']*algo_pos_merged['deta'])\n",
    "    deltar_pos=algo_pos_merged['deltar']\n",
    "    \n",
    "    #get dphi between -pi and pi\n",
    "    sel=algo_neg_merged['dphi']>np.pi\n",
    "    algo_neg_merged['dphi']-=sel*(2*np.pi)\n",
    "    algo_neg_merged['deltar']=np.sqrt(algo_neg_merged['dphi']*algo_neg_merged['dphi']+algo_neg_merged['deta']*algo_neg_merged['deta'])\n",
    "    deltar_neg=algo_neg_merged['deltar']\n",
    "    \n",
    "    #keep the unreconstructed values (NaN)\n",
    "    sel=pd.isna(algo_pos_merged['deltar']) \n",
    "    unmatched_pos=algo_pos_merged[sel]\n",
    "    sel=pd.isna(algo_neg_merged['deltar'])  \n",
    "    unmatched_neg=algo_neg_merged[sel]\n",
    "    unmatched_pos['matches']=False\n",
    "    unmatched_neg['matches']=False\n",
    "    \n",
    "    \n",
    "    #select deltar under thr\n",
    "    sel=algo_pos_merged['deltar']<=thr\n",
    "    algo_pos_merged=algo_pos_merged[sel]\n",
    "    sel=algo_neg_merged['deltar']<=thr\n",
    "    algo_neg_merged=algo_neg_merged[sel]\n",
    "    \n",
    "    #matching\n",
    "    group=algo_pos_merged.groupby('event')\n",
    "    n_rec_pos=group['cl3d_pt'].size()\n",
    "    algo_pos_merged['best_match']=group.apply(matching).array\n",
    "    group=algo_neg_merged.groupby('event')\n",
    "    n_rec_neg=group['cl3d_pt'].size()\n",
    "    algo_neg_merged['best_match']=group.apply(matching).array\n",
    "    #keep only matched clusters \n",
    "    sel=algo_pos_merged['best_match']==True\n",
    "    algo_pos_merged=algo_pos_merged[sel]\n",
    "    \n",
    "    sel=algo_neg_merged['best_match']==True\n",
    "    algo_neg_merged=algo_neg_merged[sel]\n",
    "    \n",
    "    #remerge with NaN values\n",
    "    algo_pos_merged=pd.concat([algo_pos_merged, unmatched_pos], sort=False).sort_values('event') \n",
    "    algo_neg_merged=pd.concat([algo_neg_merged, unmatched_neg], sort=False).sort_values('event')\n",
    "\n",
    "    \n",
    "    \n",
    "    deltar[i]=deltar_pos.append(deltar_neg)\n",
    "    n_rec[i]=n_rec_pos.append(n_rec_neg)\n",
    "    algo_clean[i]=pd.concat([algo_neg_merged,algo_pos_merged], sort=False).sort_values('event')\n",
    "\n",
    "    algo_clean[i]['matches']=algo_clean[i]['matches'].replace(np.nan, True)\n",
    "    algo_clean[i].drop(columns=['best_match'], inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.chdir(directory)\n",
    "os.chdir('./pion')\n",
    "gen_clean.to_csv('gen_clean.csv')\n",
    "algo_clean[0].to_csv('T23.csv')\n",
    "algo_clean[1].to_csv('SDH.csv')\n",
    "algo_clean[2].to_csv('SDH0.csv')\n",
    "algo_clean[3].to_csv('SDH10.csv')\n",
    "algo_clean[4].to_csv('SDH20.csv')\n",
    "algo_clean[5].to_csv('TDH.csv')\n",
    "algo_clean[6].to_csv('TDH0.csv')\n",
    "algo_clean[7].to_csv('TDH10.csv')\n",
    "algo_clean[8].to_csv('TDH20.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
